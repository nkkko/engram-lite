# -*- coding: utf-8 -*-
"""engramai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cb1_Xbv5O3z7giUET6ggN1g7O7d84qD6
"""

#@title Setup
# ======================================================================
# EngramAI - Knowledge Memory Graph System
# ======================================================================
#
# This module implements a cognitive architecture based on memory graphs,
# where knowledge is stored as "engrams" (knowledge units) connected in a
# semantic network. The system supports:
#
# - Creation and management of knowledge nodes (engrams)
# - Relationship modeling between knowledge units
# - Abstraction and clustering of related concepts
# - Context-aware memory retrieval
# - Integration with LLMs for knowledge generation
#
# The architecture simulates human-like memory organization with forgetting
# mechanisms, abstraction capabilities, and contextual activation.
#
# Dependencies: networkx, matplotlib, anthropic
# ======================================================================

!pip install networkx matplotlib anthropic

import os
from google.colab import userdata
# Uncomment and fill in your Anthropic API key, or set it as a secret in Colab
# os.environ["ANTHROPIC_API_KEY"] = "YOUR_API_KEY_HERE"
os.environ["ANTHROPIC_API_KEY"] = userdata.get('ANTHROPIC_API_KEY')

# Commented out IPython magic to ensure Python compatibility.
# #@title Memory Graph
# %%writefile memory_graph.py
# # memory_graph.py
# import networkx as nx
# import uuid
# import time
# import json
# from datetime import datetime
# from typing import Dict, List, Any, Optional, Set
#
# class Engram:
#     """
#     Atomic unit of knowledge/memory with metadata.
#
#     Attributes:
#         id (str): Unique identifier for the engram
#         content (str): The actual knowledge/information
#         timestamp (float): Unix timestamp of creation
#         source (str): Where this knowledge came from
#         confidence (float): Certainty score between 0.0 and 1.0
#         metadata (dict): Additional custom metadata
#     """
#     def __init__(self, content: str, source: str, confidence: float = 1.0, metadata: Dict = None):
#         self.id = str(uuid.uuid4())
#         self.content = content
#         self.timestamp = time.time()
#         self.source = source
#         self.confidence = confidence
#         self.metadata = metadata or {}
#
#     def to_dict(self) -> Dict:
#         """Convert engram to dictionary for storage/serialization"""
#         return {
#             "id": self.id,
#             "content": self.content,
#             "timestamp": self.timestamp,
#             "source": self.source,
#             "confidence": self.confidence,
#             "metadata": self.metadata
#         }
#
#     @classmethod
#     def from_dict(cls, data: Dict) -> 'Engram':
#         """Create engram from dictionary"""
#         engram = cls(
#             content=data["content"],
#             source=data["source"],
#             confidence=data["confidence"],
#             metadata=data.get("metadata", {})
#         )
#         engram.id = data["id"]
#         engram.timestamp = data["timestamp"]
#         return engram
#
#     def __repr__(self):
#         return f"Engram(id={self.id[:8]}..., content={self.content[:30]}..., confidence={self.confidence})"
#
#
# class Connection:
#     """
#     Typed relationship between engrams with strength/weight.
#
#     Attributes:
#         id (str): Unique identifier for the connection
#         source_id (str): ID of the source engram
#         target_id (str): ID of the target engram
#         relationship_type (str): Type of relationship (e.g., "causes", "supports", "contradicts")
#         weight (float): Strength of the connection (0.0 to 1.0)
#         metadata (dict): Additional custom metadata
#     """
#     def __init__(self, source_id: str, target_id: str, relationship_type: str, weight: float = 1.0, metadata: Dict = None):
#         self.id = str(uuid.uuid4())
#         self.source_id = source_id
#         self.target_id = target_id
#         self.relationship_type = relationship_type
#         self.weight = weight
#         self.metadata = metadata or {}
#
#     def to_dict(self) -> Dict:
#         """Convert connection to dictionary for storage/serialization"""
#         return {
#             "id": self.id,
#             "source_id": self.source_id,
#             "target_id": self.target_id,
#             "relationship_type": self.relationship_type,
#             "weight": self.weight,
#             "metadata": self.metadata
#         }
#
#     @classmethod
#     def from_dict(cls, data: Dict) -> 'Connection':
#         """Create connection from dictionary"""
#         connection = cls(
#             source_id=data["source_id"],
#             target_id=data["target_id"],
#             relationship_type=data["relationship_type"],
#             weight=data["weight"],
#             metadata=data.get("metadata", {})
#         )
#         connection.id = data["id"]
#         return connection
#
#     def __repr__(self):
#         return f"Connection(source={self.source_id[:8]}..., target={self.target_id[:8]}..., type={self.relationship_type}, weight={self.weight})"
#
#
# class Collection:
#     """
#     Named grouping of engrams for organization.
#
#     Attributes:
#         id (str): Unique identifier for the collection
#         name (str): Name of the collection
#         description (str): Description of what this collection represents
#         engram_ids (set): Set of engram IDs in this collection
#         metadata (dict): Additional custom metadata
#     """
#     def __init__(self, name: str, description: str = "", metadata: Dict = None):
#         self.id = str(uuid.uuid4())
#         self.name = name
#         self.description = description
#         self.engram_ids = set()
#         self.metadata = metadata or {}
#
#     def add_engram(self, engram_id: str) -> None:
#         """Add an engram to the collection"""
#         self.engram_ids.add(engram_id)
#
#     def remove_engram(self, engram_id: str) -> None:
#         """Remove an engram from the collection"""
#         if engram_id in self.engram_ids:
#             self.engram_ids.remove(engram_id)
#
#     def to_dict(self) -> Dict:
#         """Convert collection to dictionary for storage/serialization"""
#         return {
#             "id": self.id,
#             "name": self.name,
#             "description": self.description,
#             "engram_ids": list(self.engram_ids),
#             "metadata": self.metadata
#         }
#
#     @classmethod
#     def from_dict(cls, data: Dict) -> 'Collection':
#         """Create collection from dictionary"""
#         collection = cls(
#             name=data["name"],
#             description=data["description"],
#             metadata=data.get("metadata", {})
#         )
#         collection.id = data["id"]
#         collection.engram_ids = set(data["engram_ids"])
#         return collection
#
#     def __repr__(self):
#         return f"Collection(id={self.id[:8]}..., name={self.name}, engrams={len(self.engram_ids)})"
#
#
# class Agent:
#     """
#     Entity with access controls and capabilities within the system.
#
#     Attributes:
#         id (str): Unique identifier for the agent
#         name (str): Name of the agent
#         description (str): Description of the agent's role/purpose
#         capabilities (set): Set of capabilities this agent has
#         accessible_collections (set): Collection IDs this agent can access
#         metadata (dict): Additional custom metadata
#     """
#     def __init__(self, name: str, description: str = "", capabilities: Set[str] = None, metadata: Dict = None):
#         self.id = str(uuid.uuid4())
#         self.name = name
#         self.description = description
#         self.capabilities = capabilities or set()
#         self.accessible_collections = set()
#         self.metadata = metadata or {}
#
#     def grant_access(self, collection_id: str) -> None:
#         """Grant access to a collection"""
#         self.accessible_collections.add(collection_id)
#
#     def revoke_access(self, collection_id: str) -> None:
#         """Revoke access to a collection"""
#         if collection_id in self.accessible_collections:
#             self.accessible_collections.remove(collection_id)
#
#     def has_access(self, collection_id: str) -> bool:
#         """Check if agent has access to a collection"""
#         return collection_id in self.accessible_collections
#
#     def to_dict(self) -> Dict:
#         """Convert agent to dictionary for storage/serialization"""
#         return {
#             "id": self.id,
#             "name": self.name,
#             "description": self.description,
#             "capabilities": list(self.capabilities),
#             "accessible_collections": list(self.accessible_collections),
#             "metadata": self.metadata
#         }
#
#     @classmethod
#     def from_dict(cls, data: Dict) -> 'Agent':
#         """Create agent from dictionary"""
#         agent = cls(
#             name=data["name"],
#             description=data["description"],
#             capabilities=set(data["capabilities"]),
#             metadata=data.get("metadata", {})
#         )
#         agent.id = data["id"]
#         agent.accessible_collections = set(data["accessible_collections"])
#         return agent
#
#     def __repr__(self):
#         return f"Agent(id={self.id[:8]}..., name={self.name}, capabilities={len(self.capabilities)})"
#
#
# class Context:
#     """
#     Shareable environment with relevant engrams for agent collaboration.
#
#     Attributes:
#         id (str): Unique identifier for the context
#         name (str): Name of the context
#         description (str): Description of what this context represents
#         engram_ids (set): Set of engram IDs in this context
#         agent_ids (set): Set of agent IDs with access to this context
#         metadata (dict): Additional custom metadata
#     """
#     def __init__(self, name: str, description: str = "", metadata: Dict = None):
#         self.id = str(uuid.uuid4())
#         self.name = name
#         self.description = description
#         self.engram_ids = set()
#         self.agent_ids = set()
#         self.metadata = metadata or {}
#
#     def add_engram(self, engram_id: str) -> None:
#         """Add an engram to the context"""
#         self.engram_ids.add(engram_id)
#
#     def remove_engram(self, engram_id: str) -> None:
#         """Remove an engram from the context"""
#         if engram_id in self.engram_ids:
#             self.engram_ids.remove(engram_id)
#
#     def add_agent(self, agent_id: str) -> None:
#         """Add an agent to the context"""
#         self.agent_ids.add(agent_id)
#
#     def remove_agent(self, agent_id: str) -> None:
#         """Remove an agent from the context"""
#         if agent_id in self.agent_ids:
#             self.agent_ids.remove(agent_id)
#
#     def to_dict(self) -> Dict:
#         """Convert context to dictionary for storage/serialization"""
#         return {
#             "id": self.id,
#             "name": self.name,
#             "description": self.description,
#             "engram_ids": list(self.engram_ids),
#             "agent_ids": list(self.agent_ids),
#             "metadata": self.metadata
#         }
#
#     @classmethod
#     def from_dict(cls, data: Dict) -> 'Context':
#         """Create context from dictionary"""
#         context = cls(
#             name=data["name"],
#             description=data["description"],
#             metadata=data.get("metadata", {})
#         )
#         context.id = data["id"]
#         context.engram_ids = set(data["engram_ids"])
#         context.agent_ids = set(data["agent_ids"])
#         return context
#
#     def __repr__(self):
#         return f"Context(id={self.id[:8]}..., name={self.name}, engrams={len(self.engram_ids)}, agents={len(self.agent_ids)})"

# Commented out IPython magic to ensure Python compatibility.
# #@title Graph DB
# %%writefile graph_db.py
# # graph_db.py
# import networkx as nx
# from typing import Dict, List, Any, Optional, Set, Tuple
# import json
# import os
# from memory_graph import Engram, Connection, Collection, Agent, Context
#
# class MemoryGraph:
#     """
#     A graph database implementation for storing and querying engrams and their connections.
#
#     This class provides a hyper-low-latency shared space for multi-agentic AI collaboration.
#     """
#     def __init__(self):
#         # Initialize the graph
#         self.graph = nx.MultiDiGraph()
#
#         # Store objects by ID
#         self.engrams = {}
#         self.connections = {}
#         self.collections = {}
#         self.agents = {}
#         self.contexts = {}
#
#     def add_engram(self, engram: Engram) -> str:
#         """
#         Add an engram to the graph.
#
#         Args:
#             engram: The Engram object to add
#
#         Returns:
#             str: The ID of the added engram
#         """
#         # Add to internal dictionary
#         self.engrams[engram.id] = engram
#
#         # Add node to graph with all attributes
#         self.graph.add_node(
#             engram.id,
#             type='engram',
#             content=engram.content,
#             timestamp=engram.timestamp,
#             source=engram.source,
#             confidence=engram.confidence,
#             metadata=engram.metadata
#         )
#
#         return engram.id
#
#     def add_connection(self, connection: Connection) -> str:
#         """
#         Add a connection between two engrams.
#
#         Args:
#             connection: The Connection object to add
#
#         Returns:
#             str: The ID of the added connection
#         """
#         # Verify that both engrams exist
#         if connection.source_id not in self.engrams or connection.target_id not in self.engrams:
#             raise ValueError("Both source and target engrams must exist in the graph")
#
#         # Add to internal dictionary
#         self.connections[connection.id] = connection
#
#         # Add edge to graph
#         self.graph.add_edge(
#             connection.source_id,
#             connection.target_id,
#             key=connection.id,
#             type='connection',
#             relationship_type=connection.relationship_type,
#             weight=connection.weight,
#             metadata=connection.metadata
#         )
#
#         return connection.id
#
#     def add_collection(self, collection: Collection) -> str:
#         """
#         Add a collection to the graph.
#
#         Args:
#             collection: The Collection object to add
#
#         Returns:
#             str: The ID of the added collection
#         """
#         # Add to internal dictionary
#         self.collections[collection.id] = collection
#
#         # Add node to graph
#         self.graph.add_node(
#             collection.id,
#             type='collection',
#             name=collection.name,
#             description=collection.description,
#             metadata=collection.metadata
#         )
#
#         # Connect collection to its engrams
#         for engram_id in collection.engram_ids:
#             if engram_id in self.engrams:
#                 # Create a virtual edge from collection to engram
#                 self.graph.add_edge(
#                     collection.id,
#                     engram_id,
#                     key=f"collection_{collection.id}_contains_{engram_id}",
#                     type='contains'
#                 )
#
#         return collection.id
#
#     def add_agent(self, agent: Agent) -> str:
#         """
#         Add an agent to the graph.
#
#         Args:
#             agent: The Agent object to add
#
#         Returns:
#             str: The ID of the added agent
#         """
#         # Add to internal dictionary
#         self.agents[agent.id] = agent
#
#         # Add node to graph
#         self.graph.add_node(
#             agent.id,
#             type='agent',
#             name=agent.name,
#             description=agent.description,
#             capabilities=list(agent.capabilities),
#             metadata=agent.metadata
#         )
#
#         # Connect agent to its accessible collections
#         for collection_id in agent.accessible_collections:
#             if collection_id in self.collections:
#                 # Create a virtual edge from agent to collection
#                 self.graph.add_edge(
#                     agent.id,
#                     collection_id,
#                     key=f"agent_{agent.id}_access_{collection_id}",
#                     type='has_access'
#                 )
#
#         return agent.id
#
#     def add_context(self, context: Context) -> str:
#         """
#         Add a context to the graph.
#
#         Args:
#             context: The Context object to add
#
#         Returns:
#             str: The ID of the added context
#         """
#         # Add to internal dictionary
#         self.contexts[context.id] = context
#
#         # Add node to graph
#         self.graph.add_node(
#             context.id,
#             type='context',
#             name=context.name,
#             description=context.description,
#             metadata=context.metadata
#         )
#
#         # Connect context to its engrams
#         for engram_id in context.engram_ids:
#             if engram_id in self.engrams:
#                 self.graph.add_edge(
#                     context.id,
#                     engram_id,
#                     key=f"context_{context.id}_contains_{engram_id}",
#                     type='contains'
#                 )
#
#         # Connect context to its agents
#         for agent_id in context.agent_ids:
#             if agent_id in self.agents:
#                 self.graph.add_edge(
#                     context.id,
#                     agent_id,
#                     key=f"context_{context.id}_includes_{agent_id}",
#                     type='includes'
#                 )
#
#                 # Also connect in reverse for easier querying
#                 self.graph.add_edge(
#                     agent_id,
#                     context.id,
#                     key=f"agent_{agent_id}_participates_{context.id}",
#                     type='participates_in'
#                 )
#
#         return context.id
#
#     def get_engram(self, engram_id: str) -> Optional[Engram]:
#         """Get an engram by ID"""
#         return self.engrams.get(engram_id)
#
#     def get_connection(self, connection_id: str) -> Optional[Connection]:
#         """Get a connection by ID"""
#         return self.connections.get(connection_id)
#
#     def get_collection(self, collection_id: str) -> Optional[Collection]:
#         """Get a collection by ID"""
#         return self.collections.get(collection_id)
#
#     def get_agent(self, agent_id: str) -> Optional[Agent]:
#         """Get an agent by ID"""
#         return self.agents.get(agent_id)
#
#     def get_context(self, context_id: str) -> Optional[Context]:
#         """Get a context by ID"""
#         return self.contexts.get(context_id)
#
#     def get_connections_between(self, source_id: str, target_id: str) -> List[Connection]:
#         """Get all connections between two engrams"""
#         connections = []
#         if self.graph.has_edge(source_id, target_id):
#             for key in self.graph[source_id][target_id]:
#                 edge_data = self.graph[source_id][target_id][key]
#                 if edge_data.get('type') == 'connection':
#                     connection_id = key
#                     connections.append(self.connections[connection_id])
#         return connections
#
#     def get_engrams_by_source(self, source: str) -> List[Engram]:
#         """Get all engrams from a specific source"""
#         return [engram for engram in self.engrams.values() if engram.source == source]
#
#     def get_engrams_by_confidence(self, min_confidence: float) -> List[Engram]:
#         """Get all engrams with confidence above or equal to the minimum"""
#         return [engram for engram in self.engrams.values() if engram.confidence >= min_confidence]
#
#     def get_recent_engrams(self, count: int = 10) -> List[Engram]:
#         """Get the most recent engrams"""
#         return sorted(self.engrams.values(), key=lambda e: e.timestamp, reverse=True)[:count]
#
#     def get_agent_accessible_engrams(self, agent_id: str) -> List[Engram]:
#         """Get all engrams an agent can access through collections"""
#         if agent_id not in self.agents:
#             return []
#
#         agent = self.agents[agent_id]
#         accessible_engrams = []
#
#         for collection_id in agent.accessible_collections:
#             if collection_id in self.collections:
#                 collection = self.collections[collection_id]
#                 for engram_id in collection.engram_ids:
#                     if engram_id in self.engrams:
#                         accessible_engrams.append(self.engrams[engram_id])
#
#         return accessible_engrams
#
#     def get_context_engrams(self, context_id: str) -> List[Engram]:
#         """Get all engrams in a context"""
#         if context_id not in self.contexts:
#             return []
#
#         context = self.contexts[context_id]
#         return [self.engrams[engram_id] for engram_id in context.engram_ids if engram_id in self.engrams]
#
#     def get_agents_in_context(self, context_id: str) -> List[Agent]:
#         """Get all agents in a context"""
#         if context_id not in self.contexts:
#             return []
#
#         context = self.contexts[context_id]
#         return [self.agents[agent_id] for agent_id in context.agent_ids if agent_id in self.agents]
#
#     def add_engram_to_collection(self, engram_id: str, collection_id: str) -> bool:
#         """Add an engram to a collection"""
#         if engram_id not in self.engrams or collection_id not in self.collections:
#             return False
#
#         collection = self.collections[collection_id]
#         collection.add_engram(engram_id)
#
#         # Add edge in graph
#         self.graph.add_edge(
#             collection_id,
#             engram_id,
#             key=f"collection_{collection_id}_contains_{engram_id}",
#             type='contains'
#         )
#
#         return True
#
#     def add_engram_to_context(self, engram_id: str, context_id: str) -> bool:
#         """Add an engram to a context"""
#         if engram_id not in self.engrams or context_id not in self.contexts:
#             return False
#
#         context = self.contexts[context_id]
#         context.add_engram(engram_id)
#
#         # Add edge in graph
#         self.graph.add_edge(
#             context_id,
#             engram_id,
#             key=f"context_{context_id}_contains_{engram_id}",
#             type='contains'
#         )
#
#         return True
#
#     def add_agent_to_context(self, agent_id: str, context_id: str) -> bool:
#         """Add an agent to a context"""
#         if agent_id not in self.agents or context_id not in self.contexts:
#             return False
#
#         context = self.contexts[context_id]
#         context.add_agent(agent_id)
#
#         # Add edges in graph (bidirectional for easier querying)
#         self.graph.add_edge(
#             context_id,
#             agent_id,
#             key=f"context_{context_id}_includes_{agent_id}",
#             type='includes'
#         )
#
#         self.graph.add_edge(
#             agent_id,
#             context_id,
#             key=f"agent_{agent_id}_participates_{context_id}",
#             type='participates_in'
#         )
#
#         return True
#
#     def save_to_file(self, filename: str) -> None:
#         """Save the entire graph database to a file"""
#         data = {
#             "engrams": {id: engram.to_dict() for id, engram in self.engrams.items()},
#             "connections": {id: connection.to_dict() for id, connection in self.connections.items()},
#             "collections": {id: collection.to_dict() for id, collection in self.collections.items()},
#             "agents": {id: agent.to_dict() for id, agent in self.agents.items()},
#             "contexts": {id: context.to_dict() for id, context in self.contexts.items()}
#         }
#
#         with open(filename, 'w') as f:
#             json.dump(data, f, indent=2)
#
#     @classmethod
#     def load_from_file(cls, filename: str) -> 'MemoryGraph':
#         """Load a graph database from a file"""
#         with open(filename, 'r') as f:
#             data = json.load(f)
#
#         graph_db = cls()
#
#         # Load engrams first
#         for id, engram_data in data.get("engrams", {}).items():
#             engram = Engram.from_dict(engram_data)
#             graph_db.add_engram(engram)
#
#         # Load connections
#         for id, connection_data in data.get("connections", {}).items():
#             connection = Connection.from_dict(connection_data)
#             try:
#                 graph_db.add_connection(connection)
#             except ValueError:
#                 # Skip connections that reference non-existent engrams
#                 pass
#
#         # Load collections
#         for id, collection_data in data.get("collections", {}).items():
#             collection = Collection.from_dict(collection_data)
#             graph_db.add_collection(collection)
#
#         # Load agents
#         for id, agent_data in data.get("agents", {}).items():
#             agent = Agent.from_dict(agent_data)
#             graph_db.add_agent(agent)
#
#         # Load contexts
#         for id, context_data in data.get("contexts", {}).items():
#             context = Context.from_dict(context_data)
#             graph_db.add_context(context)
#
#         return graph_db
#
#     def visualize(self, save_path=None):
#         """Visualize the graph using matplotlib"""
#         try:
#             import matplotlib.pyplot as plt
#
#             # Create position layout
#             pos = nx.spring_layout(self.graph)
#
#             # Create node color map
#             node_colors = []
#             for node in self.graph.nodes():
#                 node_type = self.graph.nodes[node].get('type', '')
#                 if node_type == 'engram':
#                     node_colors.append('skyblue')
#                 elif node_type == 'collection':
#                     node_colors.append('lightgreen')
#                 elif node_type == 'agent':
#                     node_colors.append('salmon')
#                 elif node_type == 'context':
#                     node_colors.append('yellow')
#                 else:
#                     node_colors.append('gray')
#
#             # Draw the graph
#             fig = plt.figure(figsize=(12, 8))
#             nx.draw(self.graph, pos, with_labels=True, node_color=node_colors,
#                   node_size=500, font_size=8, font_weight='bold',
#                   edge_color='gray', arrows=True)
#
#             # Add a legend
#             legend_elements = [
#                 plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='skyblue', markersize=10, label='Engram'),
#                 plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=10, label='Collection'),
#                 plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='salmon', markersize=10, label='Agent'),
#                 plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='yellow', markersize=10, label='Context')
#             ]
#             plt.legend(handles=legend_elements, loc='upper right')
#
#             plt.title('Memory Graph Visualization')
#             plt.axis('off')
#
#             # Save to file if path provided
#             if save_path:
#                 plt.savefig(save_path, bbox_inches='tight')
#                 print(f"Visualization saved to {save_path}")
#
#             # Display in notebook
#             plt.show()
#
#             return fig
#         except ImportError:
#             print("Visualization requires matplotlib. Please install it with 'pip install matplotlib'")
#             return None

# Commented out IPython magic to ensure Python compatibility.
# #@title Data Generator
# %%writefile data_generator.py
# # data_generator.py
# import random
# import time
# from typing import List, Dict, Any, Optional
# import anthropic
# import os
# from memory_graph import Engram, Connection, Collection, Agent, Context
#
# class DataGenerator:
#     """
#     Utility class for generating synthetic data for the memory graph,
#     including integration with the Anthropic API for content generation.
#     """
#     def __init__(self, anthropic_api_key: Optional[str] = None):
#         """
#         Initialize the data generator.
#
#         Args:
#             anthropic_api_key: Anthropic API key for Claude access
#         """
#         self.anthropic_api_key = anthropic_api_key or os.environ.get("ANTHROPIC_API_KEY")
#         if self.anthropic_api_key:
#             self.client = anthropic.Anthropic(api_key=self.anthropic_api_key)
#         else:
#             self.client = None
#
#     def generate_knowledge_with_llm(self, topic: str, count: int = 5) -> List[str]:
#         """
#         Generate knowledge about a topic using Anthropic's Claude API.
#
#         Args:
#             topic: The topic to generate knowledge about
#             count: Number of knowledge points to generate
#
#         Returns:
#             List of generated knowledge statements
#         """
#         if not self.client:
#             raise ValueError("Anthropic API key not provided")
#
#         prompt = f"""
#         Generate {count} distinct knowledge points about {topic}.
#         Format each point as a concise, factual statement.
#         Return only the numbered list of statements, nothing else.
#         """
#
#         try:
#             message = self.client.messages.create(
#                 model="claude-3-5-haiku-latest",
#                 max_tokens=1000,
#                 temperature=0.7,
#                 system="You are a helpful AI that generates concise, accurate knowledge statements.",
#                 messages=[{"role": "user", "content": prompt}]
#             )
#             content = message.content[0].text
#
#             # Parse content to extract statements
#             lines = content.strip().split('\n')
#             statements = []
#             for line in lines:
#                 # Strip numbering and whitespace
#                 if '.' in line:
#                     _, statement = line.split('.', 1)
#                     statements.append(statement.strip())
#                 else:
#                     statements.append(line.strip())
#
#             # Filter out empty statements and limit to requested count
#             statements = [s for s in statements if s][:count]
#             return statements
#
#         except Exception as e:
#             print(f"Error generating knowledge with Anthropic API: {e}")
#             # Fallback to simple synthetic data
#             return [f"Fact about {topic} #{i+1}" for i in range(count)]
#
#     def create_synthetic_engrams(self, topics: List[str], per_topic: int = 3) -> List[Engram]:
#         """
#         Create synthetic engrams about multiple topics.
#
#         Args:
#             topics: List of topics to generate knowledge about
#             per_topic: Number of engrams per topic
#
#         Returns:
#             List of generated Engram objects
#         """
#         engrams = []
#         sources = ["research", "observation", "inference", "user_input", "external_api"]
#
#         for topic in topics:
#             try:
#                 if self.client:
#                     # Try to use the LLM first
#                     contents = self.generate_knowledge_with_llm(topic, per_topic)
#                 else:
#                     # Fallback to simple synthetic data
#                     contents = [f"Fact about {topic} #{i+1}" for i in range(per_topic)]
#             except Exception as e:
#                 print(f"Error generating content for {topic}: {e}")
#                 contents = [f"Fact about {topic} #{i+1}" for i in range(per_topic)]
#
#             for content in contents:
#                 source = random.choice(sources)
#                 confidence = round(random.uniform(0.7, 1.0), 2)
#                 metadata = {
#                     "topic": topic,
#                     "generated_timestamp": time.time(),
#                     "keywords": [topic] + random.sample(["important", "verified", "uncertain", "critical"],
#                                                       k=random.randint(0, 3))
#                 }
#
#                 engram = Engram(content=content, source=source, confidence=confidence, metadata=metadata)
#                 engrams.append(engram)
#
#         return engrams
#
#     def create_connections_between_engrams(self, engrams: List[Engram]) -> List[Connection]:
#         """
#         Create random connections between engrams.
#
#         Args:
#             engrams: List of engrams to connect
#
#         Returns:
#             List of generated Connection objects
#         """
#         connections = []
#         relationship_types = ["supports", "contradicts", "elaborates", "references", "causes", "follows"]
#
#         # Create connections, ensuring some related by topic
#         for i, engram in enumerate(engrams):
#             # Determine number of connections to create from this engram
#             num_connections = random.randint(1, min(3, len(engrams) - 1))
#
#             # Find engrams with same topic first
#             same_topic_engrams = [e for e in engrams if e.id != engram.id and
#                                  e.metadata.get("topic") == engram.metadata.get("topic")]
#
#             # Select some target engrams, preferring same topic but allowing others
#             target_engrams = []
#             if same_topic_engrams:
#                 # Add 1-2 same-topic connections if available
#                 num_same_topic = min(len(same_topic_engrams), random.randint(1, 2))
#                 target_engrams.extend(random.sample(same_topic_engrams, num_same_topic))
#
#             # Fill remaining connections with random engrams
#             other_engrams = [e for e in engrams if e.id != engram.id and e not in target_engrams]
#             if other_engrams and len(target_engrams) < num_connections:
#                 remaining = num_connections - len(target_engrams)
#                 target_engrams.extend(random.sample(other_engrams, min(remaining, len(other_engrams))))
#
#             # Create the connections
#             for target in target_engrams:
#                 relationship_type = random.choice(relationship_types)
#
#                 # Determine weight based on relationship and confidence
#                 if relationship_type in ["supports", "elaborates"]:
#                     # Higher weight for supportive relationships
#                     weight = round(random.uniform(0.7, 1.0), 2)
#                 else:
#                     # Lower weight for contradictory or uncertain relationships
#                     weight = round(random.uniform(0.3, 0.8), 2)
#
#                 # Adjust weight by source confidence
#                 weight = weight * engram.confidence
#
#                 metadata = {
#                     "created_at": time.time(),
#                     "reasoning": f"Connection established based on {relationship_type} relationship"
#                 }
#
#                 connection = Connection(
#                     source_id=engram.id,
#                     target_id=target.id,
#                     relationship_type=relationship_type,
#                     weight=weight,
#                     metadata=metadata
#                 )
#                 connections.append(connection)
#
#         return connections
#
#     def create_collections(self, engrams: List[Engram]) -> List[Collection]:
#         """
#         Group engrams into meaningful collections.
#
#         Args:
#             engrams: List of engrams to organize
#
#         Returns:
#             List of Collection objects
#         """
#         collections = []
#
#         # Group by topic
#         topics = {}
#         for engram in engrams:
#             topic = engram.metadata.get("topic", "miscellaneous")
#             if topic not in topics:
#                 topics[topic] = []
#             topics[topic].append(engram.id)
#
#         # Create a collection for each topic
#         for topic, engram_ids in topics.items():
#             collection = Collection(
#                 name=f"{topic.capitalize()} Knowledge",
#                 description=f"Collection of engrams about {topic}",
#                 metadata={"topic": topic, "size": len(engram_ids)}
#             )
#             for engram_id in engram_ids:
#                 collection.add_engram(engram_id)
#             collections.append(collection)
#
#         # Add a few special collections for high-confidence or source-based groupings
#         high_confidence_ids = [e.id for e in engrams if e.confidence >= 0.9]
#         if high_confidence_ids:
#             collection = Collection(
#                 name="High Confidence Facts",
#                 description="Engrams with very high confidence scores",
#                 metadata={"filter": "confidence >= 0.9"}
#             )
#             for engram_id in high_confidence_ids:
#                 collection.add_engram(engram_id)
#             collections.append(collection)
#
#         # Group by source
#         sources = {}
#         for engram in engrams:
#             source = engram.source
#             if source not in sources:
#                 sources[source] = []
#             sources[source].append(engram.id)
#
#         for source, engram_ids in sources.items():
#             collection = Collection(
#                 name=f"{source.capitalize()} Source",
#                 description=f"Engrams from {source} source",
#                 metadata={"source": source}
#             )
#             for engram_id in engram_ids:
#                 collection.add_engram(engram_id)
#             collections.append(collection)
#
#         return collections
#
#     def create_agents(self, collections: List[Collection]) -> List[Agent]:
#         """
#         Create agents with different roles and capabilities.
#
#         Args:
#             collections: List of collections for setting access rights
#
#         Returns:
#             List of Agent objects
#         """
#         # Define agent templates
#         agent_templates = [
#             {
#                 "name": "Developer",
#                 "description": "Specializes in writing clean, efficient code based on requirements",
#                 "capabilities": {"code", "implement", "refactor", "debug", "query"}
#             },
#             {
#                 "name": "Code Reviewer",
#                 "description": "Evaluates code for quality, readability, and adherence to standards",
#                 "capabilities": {"analyze", "evaluate", "suggest", "query", "reference"}
#             },
#             {
#                 "name": "Architect",
#                 "description": "Designs high-level code structure and system components",
#                 "capabilities": {"design", "plan", "optimize", "query", "model"}
#             },
#             {
#                 "name": "Tester",
#                 "description": "Creates test cases and validates code functionality",
#                 "capabilities": {"test", "validate", "edge-case", "coverage", "query"}
#             },
#             {
#                 "name": "Documenter",
#                 "description": "Creates clear documentation for code and APIs",
#                 "capabilities": {"document", "explain", "organize", "query", "standardize"}
#             },
#             {
#                 "name": "Debugger",
#                 "description": "Specializes in identifying and resolving code issues",
#                 "capabilities": {"trace", "diagnose", "fix", "query", "reproduce"}
#             },
#             {
#                 "name": "Optimizer",
#                 "description": "Improves code performance, efficiency, and resource usage",
#                 "capabilities": {"profile", "benchmark", "optimize", "query", "measure"}
#             }
#         ]
#
#         agents = []
#
#         # Create agents based on templates
#         for template in agent_templates:
#             agent = Agent(
#                 name=template["name"],
#                 description=template["description"],
#                 capabilities=template["capabilities"],
#                 metadata={"role": template["name"].lower()}
#             )
#
#             # Assign collections based on agent role
#             if agent.name == "Developer":
#                 # Developer gets access to code libraries, frameworks, and implementation examples
#                 for collection in collections:
#                     if any(term in collection.name.lower() for term in ["code", "library", "framework", "implementation", "examples"]):
#                         agent.grant_access(collection.id)
#
#             elif agent.name == "Code Reviewer":
#                 # Code Reviewer gets access to standards, patterns, and best practices
#                 for collection in collections:
#                     if any(term in collection.name.lower() for term in ["standards", "patterns", "best practices", "quality", "review"]):
#                         agent.grant_access(collection.id)
#
#             elif agent.name == "Architect":
#                 # Architect gets access to design patterns and system architecture resources
#                 for collection in collections:
#                     if any(term in collection.name.lower() for term in ["architecture", "design patterns", "system design", "scalability"]):
#                         agent.grant_access(collection.id)
#
#             elif agent.name == "Tester":
#                 # Tester gets access to testing methodologies and QA resources
#                 for collection in collections:
#                     if any(term in collection.name.lower() for term in ["testing", "qa", "validation", "test cases", "coverage"]):
#                         agent.grant_access(collection.id)
#
#             elif agent.name == "Documenter":
#                 # Documenter gets access to documentation standards and examples
#                 for collection in collections:
#                     if any(term in collection.name.lower() for term in ["documentation", "api docs", "technical writing", "comments"]):
#                         agent.grant_access(collection.id)
#
#             elif agent.name == "Debugger":
#                 # Debugger gets access to troubleshooting resources and common issues
#                 for collection in collections:
#                     if any(term in collection.name.lower() for term in ["debugging", "issues", "troubleshooting", "fixes", "bugs"]):
#                         agent.grant_access(collection.id)
#
#             elif agent.name == "Optimizer":
#                 # Optimizer gets access to performance optimization resources
#                 for collection in collections:
#                     if any(term in collection.name.lower() for term in ["performance", "optimization", "benchmarks", "efficiency"]):
#                         agent.grant_access(collection.id)
#
#             # All coding agents get access to general programming knowledge
#             for collection in collections:
#                 if collection.name.lower().startswith("programming") or "fundamentals" in collection.name.lower():
#                     agent.grant_access(collection.id)
#
#             # Architect and Developer both need access to security resources
#             if agent.name in ["Architect", "Developer"]:
#                 for collection in collections:
#                     if "security" in collection.name.lower():
#                         agent.grant_access(collection.id)
#
#             agents.append(agent)
#
#         return agents
#
#     def create_contexts(self, engrams: List[Engram], agents: List[Agent]) -> List[Context]:
#         """
#         Create collaboration contexts for agents.
#
#         Args:
#             engrams: List of all engrams
#             agents: List of all agents
#
#         Returns:
#             List of Context objects
#         """
#         contexts = []
#
#         # Define task type mapping - maps topics to relevant task types
#         # This fixes the undefined task_type variable issue in the original code
#         task_type_mapping = {
#             "artificial intelligence": "implementation",
#             "machine learning": "implementation",
#             "deep learning": "implementation",
#             "neural networks": "implementation",
#             "code quality": "review",
#             "code standards": "review",
#             "best practices": "review",
#             "software architecture": "design",
#             "system design": "design",
#             "design patterns": "design",
#             "testing methodologies": "testing",
#             "test cases": "testing",
#             "unit testing": "testing",
#             "documentation": "documentation",
#             "api design": "documentation",
#             "debugging": "debug",
#             "error handling": "debug",
#             "code optimization": "performance",
#             "performance tuning": "performance",
#             "algorithms": "implementation",
#             "data structures": "implementation",
#             "security": "implementation",
#             "cloud computing": "design",
#             "frontend": "implementation",
#             "backend": "implementation",
#             "database": "implementation",
#             "devops": "implementation",
#             # Default for other topics
#             "default": "complex_project"
#         }
#
#         # Group engrams by topic
#         topics = {}
#         for engram in engrams:
#             topic = engram.metadata.get("topic", "miscellaneous")
#             if topic not in topics:
#                 topics[topic] = []
#             topics[topic].append(engram.id)
#
#         # Create a context for each major topic
#         for topic, engram_ids in topics.items():
#             # Take a subset of engrams for the context
#             selected_engram_ids = random.sample(engram_ids, min(len(engram_ids), 5))
#
#             # Determine the task type for this topic
#             task_type = task_type_mapping.get(topic.lower(), task_type_mapping["default"])
#
#             # Select agents that might be interested in this topic
#             selected_agents = []
#             for agent in agents:
#                 # Different selection logic based on coding agent type and task type
#                 if agent.name == "Developer" and task_type in ["implementation", "feature", "refactoring"]:
#                     selected_agents.append(agent.id)
#                 elif agent.name == "Code Reviewer" and task_type in ["review", "quality_check", "standards"]:
#                     selected_agents.append(agent.id)
#                 elif agent.name == "Architect" and task_type in ["design", "planning", "system_structure"]:
#                     selected_agents.append(agent.id)
#                 elif agent.name == "Tester" and task_type in ["testing", "validation", "qa"]:
#                     selected_agents.append(agent.id)
#                 elif agent.name == "Documenter" and task_type in ["documentation", "comments", "api_docs"]:
#                     selected_agents.append(agent.id)
#                 elif agent.name == "Debugger" and task_type in ["debug", "issues", "errors", "bugs"]:
#                     selected_agents.append(agent.id)
#                 elif agent.name == "Optimizer" and task_type in ["performance", "efficiency", "optimization"]:
#                     selected_agents.append(agent.id)
#
#                 # Include special case for complex tasks requiring multiple specialists
#                 if task_type == "complex_project":
#                     # For complex projects, include Developer, Architect, and Tester by default
#                     if agent.name in ["Developer", "Architect", "Tester"]:
#                         selected_agents.append(agent.id)
#
#                 # Code Reviewer is included in all implementation tasks for quality assurance
#                 if agent.name == "Code Reviewer" and task_type in ["implementation", "feature", "refactoring"]:
#                     if agent.id not in selected_agents:
#                         selected_agents.append(agent.id)
#
#             context = Context(
#                 name=f"{topic.capitalize()} Discussion",
#                 description=f"Collaborative context for discussing {topic}",
#                 metadata={"topic": topic, "purpose": "collaboration", "task_type": task_type}
#             )
#
#             # Add selected engrams and agents
#             for engram_id in selected_engram_ids:
#                 context.add_engram(engram_id)
#
#             for agent_id in selected_agents:
#                 context.add_agent(agent_id)
#
#             contexts.append(context)
#
#         # Create a general context with high-confidence engrams and all agents
#         high_confidence_ids = [e.id for e in engrams if e.confidence >= 0.9]
#         if high_confidence_ids:
#             # Take up to 10 high-confidence engrams
#             selected_engram_ids = random.sample(high_confidence_ids, min(len(high_confidence_ids), 10))
#
#             general_context = Context(
#                 name="General Knowledge Sharing",
#                 description="Context for sharing important verified information across domains",
#                 metadata={"type": "general", "importance": "high", "task_type": "knowledge_sharing"}
#             )
#
#             for engram_id in selected_engram_ids:
#                 general_context.add_engram(engram_id)
#
#             for agent in agents:
#                 general_context.add_agent(agent.id)
#
#             contexts.append(general_context)
#
#         return contexts
#
#     def generate_complete_memory_graph(self, topics: List[str], engrams_per_topic: int = 5) -> Dict[str, List]:
#         """
#         Generate a complete memory graph with all components.
#
#         Args:
#             topics: List of topics to generate knowledge about
#             engrams_per_topic: Number of engrams to generate per topic
#
#         Returns:
#             Dictionary containing all generated components
#         """
#         result = {}
#
#         # Generate engrams
#         print("Generating engrams...")
#         engrams = self.create_synthetic_engrams(topics, engrams_per_topic)
#         result["engrams"] = engrams
#
#         # Generate connections
#         print("Creating connections between engrams...")
#         connections = self.create_connections_between_engrams(engrams)
#         result["connections"] = connections
#
#         # Generate collections
#         print("Organizing engrams into collections...")
#         collections = self.create_collections(engrams)
#         result["collections"] = collections
#
#         # Generate agents
#         print("Creating specialized agents...")
#         agents = self.create_agents(collections)
#         result["agents"] = agents
#
#         # Generate contexts
#         print("Establishing collaboration contexts...")
#         contexts = self.create_contexts(engrams, agents)
#         result["contexts"] = contexts
#
#         print(f"Memory graph generation complete with {len(engrams)} engrams, "
#               f"{len(connections)} connections, {len(collections)} collections, "
#               f"{len(agents)} agents, and {len(contexts)} contexts.")
#
#         return result

# Commented out IPython magic to ensure Python compatibility.
# #@title Demo
# %%writefile demo.py
# # demo.py
# import os
# import random
# from memory_graph import Engram, Connection, Collection, Agent, Context
# from graph_db import MemoryGraph
# from data_generator import DataGenerator
# import matplotlib.pyplot as plt
# import networkx as nx
#
# def main():
#     # Check for API key
#     api_key = os.environ.get("ANTHROPIC_API_KEY")
#     if not api_key:
#         print("Warning: ANTHROPIC_API_KEY not found in environment variables.")
#         print("Will use synthetic data instead of generating with Claude.")
#
#     # Initialize the graph database
#     memory_graph = MemoryGraph()
#
#     # Initialize the data generator
#     generator = DataGenerator(anthropic_api_key=api_key)
#
#     # Define topics for knowledge generation
#     topics = ["artificial intelligence", "climate science", "history", "medicine", "space exploration"]
#
#     print("Generating synthetic data...")
#
#     # Generate engrams (knowledge units)
#     engrams = generator.create_synthetic_engrams(topics, per_topic=4)
#     print(f"Created {len(engrams)} engrams across {len(topics)} topics")
#
#     # Add engrams to the graph
#     for engram in engrams:
#         memory_graph.add_engram(engram)
#
#     # Create connections between engrams
#     connections = generator.create_connections_between_engrams(engrams)
#     print(f"Created {len(connections)} connections between engrams")
#
#     # Add connections to the graph
#     for connection in connections:
#         try:
#             memory_graph.add_connection(connection)
#         except ValueError as e:
#             print(f"Error adding connection: {e}")
#
#     # Create collections
#     collections = generator.create_collections(engrams)
#     print(f"Created {len(collections)} collections")
#
#     # Add collections to the graph
#     for collection in collections:
#         memory_graph.add_collection(collection)
#
#     # Create agents
#     agents = generator.create_agents(collections)
#     print(f"Created {len(agents)} agents")
#
#     # Add agents to the graph
#     for agent in agents:
#         memory_graph.add_agent(agent)
#
#     # Create contexts for collaboration
#     contexts = generator.create_contexts(engrams, agents)
#     print(f"Created {len(contexts)} collaboration contexts")
#
#     # Add contexts to the graph
#     for context in contexts:
#         memory_graph.add_context(context)
#
#     # Demonstrate some basic queries
#     print("\nDemonstrating basic queries:")
#
#     # Get recent engrams
#     recent_engrams = memory_graph.get_recent_engrams(5)
#     print("\nRecent engrams:")
#     for engram in recent_engrams:
#         print(f"- {engram.content[:50]}... (confidence: {engram.confidence})")
#
#     # Get high confidence engrams
#     high_confidence = memory_graph.get_engrams_by_confidence(0.9)
#     print(f"\nFound {len(high_confidence)} high-confidence engrams")
#
#     # Show an example of agent-accessible knowledge
#     if agents:
#         agent = agents[0]
#         accessible_engrams = memory_graph.get_agent_accessible_engrams(agent.id)
#         print(f"\nAgent '{agent.name}' has access to {len(accessible_engrams)} engrams")
#
#     # Show context collaboration example
#     if contexts:
#         context = contexts[0]
#         context_engrams = memory_graph.get_context_engrams(context.id)
#         context_agents = memory_graph.get_agents_in_context(context.id)
#
#         print(f"\nContext: {context.name}")
#         print(f"- Contains {len(context_engrams)} engrams and {len(context_agents)} agents")
#         print("- Agent participants:")
#         for agent in context_agents:
#             print(f"  * {agent.name}: {agent.description}")
#
#         print("- Sample knowledge in this context:")
#         for engram in context_engrams[:3]:
#             print(f"  * {engram.content[:70]}...")
#
#     # Save the graph database to a file
#     memory_graph.save_to_file("memory_graph.json")
#     print("\nSaved memory graph to memory_graph.json")
#
#     # Visualize the graph
#     print("\nGenerating graph visualization...")
#     memory_graph.visualize()
#
#     print("\nDemo completed successfully!")
#
# if __name__ == "__main__":
#     main()

# Commented out IPython magic to ensure Python compatibility.
# #@title Agent Collaboration
# %%writefile agent_collaboration.py
# # agent_collaboration.py
# from memory_graph import Engram, Connection, Context
# from graph_db import MemoryGraph
# import time
# import os
# import anthropic
#
# class AIAgent:
#     """
#     Simple AI agent that can interact with the memory graph
#     and collaborate with other agents through shared contexts.
#     """
#     def __init__(self, agent_id: str, graph: MemoryGraph, api_key: str = None):
#         self.agent_id = agent_id
#         self.graph = graph
#         self.agent = graph.get_agent(agent_id)
#
#         if not self.agent:
#             raise ValueError(f"Agent with ID {agent_id} not found in the graph")
#
#         self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
#         if self.api_key:
#             self.client = anthropic.Anthropic(api_key=self.api_key)
#         else:
#             self.client = None
#             print(f"Warning: Agent {self.agent.name} initialized without LLM capabilities")
#
#     def get_agent_info(self):
#         """Return basic information about this agent"""
#         return {
#             "id": self.agent_id,
#             "name": self.agent.name,
#             "description": self.agent.description,
#             "capabilities": list(self.agent.capabilities)
#         }
#
#     def get_accessible_knowledge(self):
#         """Retrieve all engrams this agent can access"""
#         return self.graph.get_agent_accessible_engrams(self.agent_id)
#
#     def get_accessible_contexts(self):
#         """Find all contexts this agent is part of"""
#         contexts = []
#         for context_id in self.graph.contexts:
#             context = self.graph.get_context(context_id)
#             if self.agent_id in context.agent_ids:
#                 contexts.append(context)
#         return contexts
#
#     def analyze_context(self, context_id: str):
#         """Analyze the knowledge in a specific context"""
#         context = self.graph.get_context(context_id)
#         if not context:
#             return {"error": f"Context {context_id} not found"}
#
#         if self.agent_id not in context.agent_ids:
#             return {"error": f"Agent {self.agent.name} does not have access to this context"}
#
#         engrams = self.graph.get_context_engrams(context_id)
#         if not engrams:
#             return {"status": "empty", "message": "No knowledge found in this context"}
#
#         # Simple analysis without LLM
#         topics = {}
#         total_confidence = 0
#
#         for engram in engrams:
#             topic = engram.metadata.get("topic", "unknown")
#             if topic not in topics:
#                 topics[topic] = 0
#             topics[topic] += 1
#             total_confidence += engram.confidence
#
#         avg_confidence = total_confidence / len(engrams) if engrams else 0
#
#         analysis = {
#             "context_name": context.name,
#             "engram_count": len(engrams),
#             "topics": topics,
#             "average_confidence": avg_confidence,
#         }
#
#         if self.client and "analyze" in self.agent.capabilities:
#             # Use LLM for deeper analysis
#             try:
#                 content_text = "\n".join([f"- {e.content} (confidence: {e.confidence})" for e in engrams[:10]])
#
#                 prompt = f"""
#                 You are {self.agent.name}, {self.agent.description}.
#
#                 Analyze the following information from the context "{context.name}":
#
#                 {content_text}
#
#                 Provide a brief summary and identify key insights or patterns.
#                 """
#
#                 message = self.client.messages.create(
#                     model="claude-3-haiku-20240307",
#                     max_tokens=500,
#                     temperature=0.5,
#                     system=f"You are {self.agent.name}, an AI with these capabilities: {', '.join(self.agent.capabilities)}.",
#                     messages=[{"role": "user", "content": prompt}]
#                 )
#
#                 analysis["llm_insights"] = message.content[0].text
#
#             except Exception as e:
#                 analysis["llm_error"] = str(e)
#
#         return analysis
#
#     def contribute_to_context(self, context_id: str, content: str, confidence: float = 0.8):
#         """Add new knowledge to a context"""
#         context = self.graph.get_context(context_id)
#         if not context:
#             return {"error": f"Context {context_id} not found"}
#
#         if self.agent_id not in context.agent_ids:
#             return {"error": f"Agent {self.agent.name} does not have access to this context"}
#
#         # Create new engram
#         metadata = {
#             "topic": context.metadata.get("topic", "general"),
#             "contributor": self.agent.name,
#             "contribution_type": "insight"
#         }
#
#         engram = Engram(
#             content=content,
#             source=self.agent.name,
#             confidence=confidence,
#             metadata=metadata
#         )
#
#         # Add to graph
#         engram_id = self.graph.add_engram(engram)
#
#         # Add to context
#         self.graph.add_engram_to_context(engram_id, context_id)
#
#         # Try to connect to existing knowledge
#         context_engrams = self.graph.get_context_engrams(context_id)
#         for existing in context_engrams:
#             # Skip self-connection
#             if existing.id == engram_id:
#                 continue
#
#             # Connect to related engrams
#             if existing.metadata.get("topic") == metadata.get("topic"):
#                 connection = Connection(
#                     source_id=engram_id,
#                     target_id=existing.id,
#                     relationship_type="elaborates",
#                     weight=0.7,
#                     metadata={"created_by": self.agent.name}
#                 )
#                 try:
#                     self.graph.add_connection(connection)
#                 except ValueError:
#                     pass
#
#         return {
#             "status": "success",
#             "message": f"Added new engram to context {context.name}",
#             "engram_id": engram_id
#         }
#
# def simulate_collaboration():
#     """Simulate agents collaborating in a shared context"""
#     # Load existing graph or create a new one
#     try:
#         memory_graph = MemoryGraph.load_from_file("memory_graph.json")
#         print("Loaded existing memory graph")
#     except (FileNotFoundError, json.JSONDecodeError):
#         print("No valid memory graph found, run demo.py first to generate one")
#         return
#
#     # Get available agents
#     agents_data = list(memory_graph.agents.items())
#     if not agents_data:
#         print("No agents found in the memory graph")
#         return
#
#     # Create agent instances
#     agent_instances = {}
#     for agent_id, _ in agents_data:
#         try:
#             agent_instance = AIAgent(agent_id, memory_graph)
#             agent_instances[agent_id] = agent_instance
#             print(f"Initialized agent: {agent_instance.agent.name}")
#         except ValueError as e:
#             print(f"Error initializing agent {agent_id}: {e}")
#
#     if not agent_instances:
#         print("No valid agent instances could be created")
#         return
#
#     # Find contexts with multiple agents
#     collaborative_contexts = []
#     for context_id, context in memory_graph.contexts.items():
#         if len(context.agent_ids) >= 2:
#             collaborative_contexts.append(context)
#
#     if not collaborative_contexts:
#         print("No collaborative contexts found")
#         return
#
#     # Select a context for collaboration
#     context = collaborative_contexts[0]
#     print(f"\nStarting collaboration in context: {context.name}")
#     print(f"Description: {context.description}")
#
#     # Get agents in this context
#     context_agent_ids = context.agent_ids
#     context_agents = [agent_instances[agent_id] for agent_id in context_agent_ids if agent_id in agent_instances]
#
#     print(f"Participating agents: {', '.join(agent.agent.name for agent in context_agents)}")
#
#     # Each agent analyzes the context
#     print("\n== Initial Analysis ==")
#     for agent in context_agents:
#         print(f"\n{agent.agent.name}'s Analysis:")
#         analysis = agent.analyze_context(context.id)
#
#         if "llm_insights" in analysis:
#             print(analysis["llm_insights"])
#         else:
#             topics = analysis.get("topics", {})
#             topic_str = ", ".join(f"{topic}: {count}" for topic, count in topics.items())
#             print(f"Found {analysis['engram_count']} engrams across topics: {topic_str}")
#             print(f"Average confidence: {analysis['average_confidence']:.2f}")
#
#     # Each agent contributes something to the context
#     print("\n== Agent Contributions ==")
#     for agent in context_agents:
#         print(f"\n{agent.agent.name} is contributing...")
#
#         # Generate content based on agent role
#         if agent.client:
#             # Use LLM to generate content
#             try:
#                 # Get existing engrams for context
#                 existing_engrams = memory_graph.get_context_engrams(context.id)
#                 existing_content = "\n".join([f"- {e.content}" for e in existing_engrams[:5]])
#
#                 prompt = f"""
#                 You are {agent.agent.name}, {agent.agent.description}.
#
#                 Based on these existing pieces of information in the context "{context.name}":
#
#                 {existing_content}
#
#                 Generate a new insight, connection, or piece of knowledge that would be valuable
#                 to contribute. Your contribution should reflect your role and capabilities.
#
#                 Return only the content of your contribution, nothing else.
#                 """
#
#                 message = agent.client.messages.create(
#                     model="claude-3-haiku-20240307",
#                     max_tokens=300,
#                     temperature=0.7,
#                     system=f"You are {agent.agent.name}, an AI with these capabilities: {', '.join(agent.agent.capabilities)}.",
#                     messages=[{"role": "user", "content": prompt}]
#                 )
#
#                 contribution = message.content[0].text.strip()
#
#             except Exception as e:
#                 print(f"Error generating contribution with LLM: {e}")
#                 # Fallback content based on agent role
#                 if "analyze" in agent.agent.capabilities:
#                     contribution = f"Analysis shows patterns across multiple topics in this context."
#                 elif "recommend" in agent.agent.capabilities:
#                     contribution = f"Based on the available information, further research on this topic is recommended."
#                 else:
#                     contribution = f"This information should be categorized and connected to related knowledge."
#         else:
#             # Fallback content based on agent role
#             if "analyze" in agent.agent.capabilities:
#                 contribution = f"Analysis shows patterns across multiple topics in this context."
#             elif "recommend" in agent.agent.capabilities:
#                 contribution = f"Based on the available information, further research on this topic is recommended."
#             else:
#                 contribution = f"This information should be categorized and connected to related knowledge."
#
#         # Add the contribution to the context
#         result = agent.contribute_to_context(context.id, contribution)
#
#         if result.get("status") == "success":
#             print(f"Contribution: {contribution}")
#         else:
#             print(f"Failed to contribute: {result.get('error', 'Unknown error')}")
#
#     # Final state of the context
#     print("\n== Final Context State ==")
#     engrams = memory_graph.get_context_engrams(context.id)
#     print(f"Context now contains {len(engrams)} engrams")
#
#     # Display the most recent contributions
#     recent_engrams = sorted(engrams, key=lambda e: e.timestamp, reverse=True)[:5]
#     print("\nMost recent contributions:")
#     for engram in recent_engrams:
#         contributor = engram.metadata.get("contributor", engram.source)
#         print(f"- From {contributor}: {engram.content[:100]}...")
#
#     # Save updated graph
#     memory_graph.save_to_file("memory_graph_updated.json")
#     print("\nSaved updated memory graph to memory_graph_updated.json")
#
#     # Visualize the updated graph
#     print("\nGenerating updated graph visualization...")
#     memory_graph.visualize()
#
# if __name__ == "__main__":
#     simulate_collaboration()

!python demo.py

!python agent_collaboration.py

# Commented out IPython magic to ensure Python compatibility.
#@title Save Graph As Image
# Ensure matplotlib is set for inline display in notebooks
# %matplotlib inline

# Load the graph and visualize it again
from graph_db import MemoryGraph
import networkx as nx

memory_graph = MemoryGraph.load_from_file("memory_graph_updated.json")

# Explicitly save visualization to a file
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 10))

# Create position layout
pos = nx.spring_layout(memory_graph.graph)

# Create node color map
node_colors = []
for node in memory_graph.graph.nodes():
    node_type = memory_graph.graph.nodes[node].get('type', '')
    if node_type == 'engram':
        node_colors.append('skyblue')
    elif node_type == 'collection':
        node_colors.append('lightgreen')
    elif node_type == 'agent':
        node_colors.append('salmon')
    elif node_type == 'context':
        node_colors.append('yellow')
    else:
        node_colors.append('gray')

# Draw the graph
nx.draw(memory_graph.graph, pos, with_labels=True, node_color=node_colors,
       node_size=500, font_size=8, font_weight='bold',
       edge_color='gray', arrows=True)

# Add a legend
import matplotlib.lines as mlines
legend_elements = [
    mlines.Line2D([], [], marker='o', color='w', markerfacecolor='skyblue', markersize=10, label='Engram'),
    mlines.Line2D([], [], marker='o', color='w', markerfacecolor='lightgreen', markersize=10, label='Collection'),
    mlines.Line2D([], [], marker='o', color='w', markerfacecolor='salmon', markersize=10, label='Agent'),
    mlines.Line2D([], [], marker='o', color='w', markerfacecolor='yellow', markersize=10, label='Context')
]
plt.legend(handles=legend_elements, loc='upper right')

plt.title('Memory Graph Visualization')
plt.axis('off')

# Save the visualization
plt.savefig('memory_graph_viz.png', bbox_inches='tight', dpi=300)
print("Visualization saved to 'memory_graph_viz.png'")

# Display the visualization
plt.show()

#@title Plotly Interactive Graph
# Install plotly if you don't have it
!pip install plotly -q

# Load the graph
from graph_db import MemoryGraph
import networkx as nx
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd

# Load our memory graph
memory_graph = MemoryGraph.load_from_file("memory_graph.json")

# Get a position layout for the nodes
pos = nx.spring_layout(memory_graph.graph, seed=42)

# Create edge trace
edge_x = []
edge_y = []
edge_text = []

for edge in memory_graph.graph.edges(data=True):
    x0, y0 = pos[edge[0]]
    x1, y1 = pos[edge[1]]
    edge_x.extend([x0, x1, None])
    edge_y.extend([y0, y1, None])

    # Get edge attributes for hover text
    attrs = edge[2]
    edge_type = attrs.get('type', 'unknown')
    relationship = attrs.get('relationship_type', '')
    edge_info = f"Type: {edge_type}"
    if relationship:
        edge_info += f"<br>Relationship: {relationship}"
    edge_text.extend([edge_info, edge_info, None])

edge_trace = go.Scatter(
    x=edge_x, y=edge_y,
    line=dict(width=0.5, color='#888'),
    hoverinfo='text',
    text=edge_text,
    mode='lines')

# Create node trace with different node types
node_traces = {}
node_types = ['engram', 'collection', 'agent', 'context']
colors = ['skyblue', 'lightgreen', 'salmon', 'yellow']

for node_type, color in zip(node_types, colors):
    node_traces[node_type] = go.Scatter(
        x=[], y=[],
        text=[],
        mode='markers',
        hoverinfo='text',
        marker=dict(
            color=color,
            size=10,
            line_width=2))

# Add nodes to their respective traces with all metadata as hover text
for node in memory_graph.graph.nodes():
    x, y = pos[node]
    node_attrs = memory_graph.graph.nodes[node]
    node_type = node_attrs.get('type', 'unknown')

    # Skip if not a recognized type
    if node_type not in node_traces:
        continue

    node_traces[node_type]['x'] = node_traces[node_type]['x'] + (x,)
    node_traces[node_type]['y'] = node_traces[node_type]['y'] + (y,)

    # Create hover text with all metadata attributes
    hover_text = f"ID: {node[:8]}...<br>Type: {node_type}<br>"

    if 'content' in node_attrs:
        content = node_attrs['content']
        # Truncate content if too long
        if len(content) > 50:
            content = content[:50] + "..."
        hover_text += f"Content: {content}<br>"

    if 'name' in node_attrs:
        hover_text += f"Name: {node_attrs['name']}<br>"

    if 'source' in node_attrs:
        hover_text += f"Source: {node_attrs['source']}<br>"

    if 'confidence' in node_attrs:
        hover_text += f"Confidence: {node_attrs['confidence']:.2f}<br>"

    if 'timestamp' in node_attrs:
        from datetime import datetime
        timestamp = datetime.fromtimestamp(node_attrs['timestamp']).strftime('%Y-%m-%d %H:%M:%S')
        hover_text += f"Created: {timestamp}<br>"

    if 'description' in node_attrs:
        desc = node_attrs['description']
        if len(desc) > 50:
            desc = desc[:50] + "..."
        hover_text += f"Description: {desc}<br>"

    if 'capabilities' in node_attrs and node_attrs['capabilities']:
        hover_text += f"Capabilities: {', '.join(node_attrs['capabilities'])}<br>"

    # Add metadata if it exists
    if 'metadata' in node_attrs and node_attrs['metadata']:
        hover_text += f"Metadata:<br>"
        for k, v in node_attrs['metadata'].items():
            if isinstance(v, (list, dict)):
                v = str(v)
            if isinstance(v, str) and len(v) > 30:
                v = v[:30] + "..."
            hover_text += f"  - {k}: {v}<br>"

    node_traces[node_type]['text'] = node_traces[node_type]['text'] + (hover_text,)

# Create figure
fig = make_subplots(rows=1, cols=1)

# Add all traces to the figure
fig.add_trace(edge_trace)
for node_type in node_traces:
    # Skip empty traces
    if len(node_traces[node_type]['x']) > 0:
        fig.add_trace(node_traces[node_type])

# Update layout
fig.update_layout(
    title='Interactive Memory Graph Visualization',
    titlefont_size=16,
    showlegend=False,
    hovermode='closest',
    margin=dict(b=20, l=5, r=5, t=40),
    annotations=[dict(
        text="Hover over nodes to see metadata",
        showarrow=False,
        xref="paper", yref="paper",
        x=0.005, y=-0.002)],
    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
    width=900,
    height=700,
    plot_bgcolor='white'
)

# Add a legend
for i, (node_type, color) in enumerate(zip(node_types, colors)):
    fig.add_trace(go.Scatter(
        x=[None], y=[None],
        mode='markers',
        marker=dict(size=10, color=color),
        legendgroup=node_type,
        showlegend=True,
        name=node_type.capitalize()
    ))

# Show plot
fig.show()

# Save as HTML for interactive viewing later
fig.write_html("interactive_memory_graph.html")
print("Interactive visualization saved to 'interactive_memory_graph.html'")

# You can also download this file from the Colab file browser

!pip install python-louvain

# Commented out IPython magic to ensure Python compatibility.
# #@title Memory Analysis
# %%writefile memory_analysis.py
# # memory_analysis.py
# import networkx as nx
# import community as community_louvain  # Install with: pip install python-louvain
# import matplotlib.pyplot as plt
# import pandas as pd
# import numpy as np
# from collections import defaultdict
# import time
# import json
# from typing import Dict, List, Tuple, Set, Any, Optional
# from datetime import datetime, timedelta
#
# class MemoryGraphAnalyzer:
#     """
#     Analyzes memory graphs to provide insights about knowledge organization,
#     forgetting mechanisms, abstraction capabilities, and context building.
#     """
#
#     def __init__(self, graph_file: str):
#         """
#         Initialize with a memory graph JSON file.
#
#         Args:
#             graph_file: Path to the memory graph JSON file
#         """
#         # Load the graph data
#         with open(graph_file, 'r') as f:
#             self.data = json.load(f)
#
#         # Reconstruct the NetworkX graph
#         self.graph = nx.MultiDiGraph()
#
#         # Add engram nodes
#         for engram_id, engram in self.data['engrams'].items():
#             self.graph.add_node(
#                 engram_id,
#                 type='engram',
#                 content=engram['content'],
#                 timestamp=engram['timestamp'],
#                 source=engram['source'],
#                 confidence=engram['confidence'],
#                 metadata=engram.get('metadata', {})
#             )
#
#         # Add connection edges between engrams
#         for conn_id, conn in self.data['connections'].items():
#             self.graph.add_edge(
#                 conn['source_id'],
#                 conn['target_id'],
#                 key=conn_id,
#                 type='connection',
#                 relationship_type=conn['relationship_type'],
#                 weight=conn['weight'],
#                 metadata=conn.get('metadata', {})
#             )
#
#         # Add collection nodes and their connections to engrams
#         for coll_id, coll in self.data['collections'].items():
#             self.graph.add_node(
#                 coll_id,
#                 type='collection',
#                 name=coll['name'],
#                 description=coll['description'],
#                 metadata=coll.get('metadata', {})
#             )
#
#             # Connect collection to its engrams
#             for engram_id in coll['engram_ids']:
#                 if engram_id in self.data['engrams']:
#                     self.graph.add_edge(
#                         coll_id,
#                         engram_id,
#                         key=f"collection_{coll_id}_contains_{engram_id}",
#                         type='contains'
#                     )
#
#         # Create an engram-only subgraph for some analyses
#         self.engram_graph = nx.DiGraph()
#         for engram_id in self.data['engrams']:
#             self.engram_graph.add_node(engram_id)
#
#         for conn_id, conn in self.data['connections'].items():
#             if conn['source_id'] in self.data['engrams'] and conn['target_id'] in self.data['engrams']:
#                 self.engram_graph.add_edge(
#                     conn['source_id'],
#                     conn['target_id'],
#                     weight=conn['weight']
#                 )
#
#         print(f"Loaded memory graph with {len(self.data['engrams'])} engrams, "
#               f"{len(self.data['connections'])} connections, "
#               f"{len(self.data['collections'])} collections, "
#               f"{len(self.data['agents'])} agents, and "
#               f"{len(self.data['contexts'])} contexts")
#
#     def analyze_forgetting_candidates(self, recency_weight: float = 0.4,
#                                       confidence_weight: float = 0.3,
#                                       centrality_weight: float = 0.3,
#                                       top_n: int = 10) -> pd.DataFrame:
#         """
#         Identify engrams that are candidates for forgetting/pruning based on:
#         - Low centrality (structural importance)
#         - Low confidence scores
#         - Old timestamps (recency)
#
#         Args:
#             recency_weight: Weight for recency factor (0-1)
#             confidence_weight: Weight for confidence score (0-1)
#             centrality_weight: Weight for centrality measures (0-1)
#             top_n: Number of candidates to return
#
#         Returns:
#             DataFrame of engrams ranked by forgetting priority
#         """
#         print("Analyzing forgetting candidates...")
#
#         # Get centrality measures (multiple to capture different aspects of importance)
#         # in_degree centrality - how many engrams point to this one
#         in_degree = nx.in_degree_centrality(self.engram_graph)
#
#         # PageRank - recursive importance (like Google's algorithm)
#         pagerank = nx.pagerank(self.engram_graph, weight='weight')
#
#         # Betweenness - identifies bridge nodes between communities
#         betweenness = nx.betweenness_centrality(self.engram_graph, weight='weight')
#
#         # Combine centrality metrics (normalized average)
#         combined_centrality = {}
#         for node in self.engram_graph.nodes():
#             combined_centrality[node] = (
#                 in_degree.get(node, 0) +
#                 pagerank.get(node, 0) +
#                 betweenness.get(node, 0)
#             ) / 3.0
#
#         # Analyze timestamp recency
#         current_time = time.time()
#         max_age = current_time - min([e['timestamp'] for e in self.data['engrams'].values()])
#
#         # Prepare data for ranking
#         engram_data = []
#         for engram_id, engram in self.data['engrams'].items():
#             # Calculate normalized recency (1.0 = very recent, 0.0 = very old)
#             age = current_time - engram['timestamp']
#             recency = 1.0 - (age / max_age) if max_age > 0 else 1.0
#
#             # Get combined forgetting score (higher = more forgettable)
#             # Invert centrality and confidence since lower values mean more forgettable
#             forgetting_score = (
#                 recency_weight * (1.0 - recency) +
#                 confidence_weight * (1.0 - engram['confidence']) +
#                 centrality_weight * (1.0 - combined_centrality.get(engram_id, 0))
#             )
#
#             # Count references from contexts
#             context_refs = sum(1 for ctx in self.data['contexts'].values()
#                               if engram_id in ctx['engram_ids'])
#
#             engram_data.append({
#                 'engram_id': engram_id,
#                 'content': engram['content'][:50] + '...' if len(engram['content']) > 50 else engram['content'],
#                 'timestamp': datetime.fromtimestamp(engram['timestamp']).strftime('%Y-%m-%d %H:%M'),
#                 'confidence': engram['confidence'],
#                 'centrality': combined_centrality.get(engram_id, 0),
#                 'in_degree': self.engram_graph.in_degree(engram_id),
#                 'out_degree': self.engram_graph.out_degree(engram_id),
#                 'recency': recency,
#                 'context_references': context_refs,
#                 'forgetting_score': forgetting_score
#             })
#
#         # Convert to DataFrame and sort by forgetting score
#         df = pd.DataFrame(engram_data)
#         df = df.sort_values('forgetting_score', ascending=False)
#
#         print("Top candidates for forgetting:")
#         return df.head(top_n)
#
#     def identify_subgraph_abstractions(self, min_size: int = 3, max_size: int = 10) -> List[Dict]:
#         """
#         Identify subgraphs that could be collapsed into single abstract nodes.
#         Uses multiple methods including:
#         - Dense subgraphs (k-cores)
#         - Topic-based clusters
#         - Temporal sequences
#
#         Args:
#             min_size: Minimum engrams in a subgraph to consider for abstraction
#             max_size: Maximum engrams in a subgraph to consider for abstraction
#
#         Returns:
#             List of abstraction candidates with their member engrams and metadata
#         """
#         print("Identifying subgraph abstraction candidates...")
#
#         abstractions = []
#
#         # Method 1: Find k-cores (densely connected subgraphs)
#         # Start with k=3 (each node connected to at least 3 others)
#         for k in range(3, 6):
#             try:
#                 k_core = nx.k_core(self.engram_graph.to_undirected(), k=k)
#                 if min_size <= k_core.number_of_nodes() <= max_size:
#                     # Get the engram content for these nodes
#                     engrams = {node: self.data['engrams'][node]['content']
#                               for node in k_core.nodes()}
#
#                     abstractions.append({
#                         'type': f'{k}-core',
#                         'size': k_core.number_of_nodes(),
#                         'engram_ids': list(k_core.nodes()),
#                         'engrams': engrams,
#                         'density': nx.density(k_core),
#                         'abstraction_quality': 0.7 + (k * 0.05)  # Higher k = better abstraction
#                     })
#             except nx.NetworkXError:
#                 break  # No more k-cores at this level
#
#         # Method 2: Topic-based clusters (group by metadata topic)
#         topic_clusters = defaultdict(list)
#
#         for engram_id, engram in self.data['engrams'].items():
#             topic = engram.get('metadata', {}).get('topic', 'unknown')
#             topic_clusters[topic].append(engram_id)
#
#         for topic, cluster in topic_clusters.items():
#             if min_size <= len(cluster) <= max_size:
#                 # Get the subgraph for this cluster
#                 subgraph = self.engram_graph.subgraph(cluster)
#
#                 # Only consider reasonably connected subgraphs
#                 if nx.density(subgraph) > 0.2:  # At least 20% of possible connections exist
#                     engrams = {node: self.data['engrams'][node]['content']
#                               for node in subgraph.nodes()}
#
#                     abstractions.append({
#                         'type': 'topic-cluster',
#                         'topic': topic,
#                         'size': len(cluster),
#                         'engram_ids': cluster,
#                         'engrams': engrams,
#                         'density': nx.density(subgraph),
#                         'abstraction_quality': 0.5 + (nx.density(subgraph) * 0.5)  # Denser = better
#                     })
#
#         # Method 3: Temporal sequences (engrams created close together in time)
#         # Sort engrams by timestamp
#         sorted_engrams = sorted(
#             [(engram_id, engram['timestamp']) for engram_id, engram in self.data['engrams'].items()],
#             key=lambda x: x[1]
#         )
#
#         # Group into time windows (e.g., 1-hour windows)
#         time_window = 3600  # 1 hour in seconds
#         current_window = []
#         current_start = None
#
#         for engram_id, timestamp in sorted_engrams:
#             if current_start is None:
#                 current_start = timestamp
#                 current_window = [engram_id]
#             elif timestamp - current_start <= time_window:
#                 current_window.append(engram_id)
#             else:
#                 # Process the completed window
#                 if min_size <= len(current_window) <= max_size:
#                     subgraph = self.engram_graph.subgraph(current_window)
#                     # Check if there are any connections within this temporal window
#                     if subgraph.number_of_edges() > 0:
#                         engrams = {node: self.data['engrams'][node]['content']
#                                   for node in subgraph.nodes()}
#
#                         abstractions.append({
#                             'type': 'temporal-sequence',
#                             'start_time': datetime.fromtimestamp(current_start).strftime('%Y-%m-%d %H:%M'),
#                             'end_time': datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M'),
#                             'size': len(current_window),
#                             'engram_ids': current_window,
#                             'engrams': engrams,
#                             'connections': subgraph.number_of_edges(),
#                             'abstraction_quality': 0.4 + (subgraph.number_of_edges() / len(current_window) * 0.6)
#                         })
#
#                 # Start a new window
#                 current_start = timestamp
#                 current_window = [engram_id]
#
#         # Process the final window
#         if min_size <= len(current_window) <= max_size:
#             subgraph = self.engram_graph.subgraph(current_window)
#             if subgraph.number_of_edges() > 0:
#                 engrams = {node: self.data['engrams'][node]['content']
#                           for node in subgraph.nodes()}
#
#                 abstractions.append({
#                     'type': 'temporal-sequence',
#                     'start_time': datetime.fromtimestamp(current_start).strftime('%Y-%m-%d %H:%M'),
#                     'end_time': datetime.fromtimestamp(sorted_engrams[-1][1]).strftime('%Y-%m-%d %H:%M'),
#                     'size': len(current_window),
#                     'engram_ids': current_window,
#                     'engrams': engrams,
#                     'connections': subgraph.number_of_edges(),
#                     'abstraction_quality': 0.4 + (subgraph.number_of_edges() / len(current_window) * 0.6)
#                 })
#
#         # Sort by abstraction quality
#         abstractions.sort(key=lambda x: x['abstraction_quality'], reverse=True)
#
#         print(f"Found {len(abstractions)} candidate subgraphs for abstraction")
#         return abstractions
#
#     def create_abstraction_node(self, abstraction: Dict, summarize: bool = True) -> Dict:
#         """
#         Create a new abstract engram node that represents a subgraph.
#
#         Args:
#             abstraction: The abstraction candidate dictionary
#             summarize: Whether to generate a summary for the abstraction
#
#         Returns:
#             Dict representing the new abstraction node
#         """
#         # Collect all engram contents
#         contents = [self.data['engrams'][engram_id]['content']
#                    for engram_id in abstraction['engram_ids']]
#
#         # Generate a summary (this would ideally use an LLM in production)
#         if summarize:
#             # Simple summarization approach
#             summary = f"Abstract node containing {len(contents)} related engrams about "
#
#             if abstraction['type'] == 'topic-cluster':
#                 summary += f"the topic '{abstraction['topic']}'"
#             elif abstraction['type'] == 'temporal-sequence':
#                 summary += f"events from {abstraction['start_time']} to {abstraction['end_time']}"
#             else:
#                 summary += f"densely connected information (density: {abstraction['density']:.2f})"
#
#             # Add snippets from the first few engrams
#             if contents:
#                 summary += "\n\nIncluding: \n"
#                 for i, content in enumerate(contents[:3]):
#                     if len(content) > 100:
#                         content = content[:100] + "..."
#                     summary += f"- {content}\n"
#
#                 if len(contents) > 3:
#                     summary += f"- And {len(contents) - 3} more items"
#         else:
#             # Just concatenate with minimal formatting
#             summary = "\n\n".join(contents)
#
#         # Calculate average confidence
#         avg_confidence = sum(self.data['engrams'][engram_id]['confidence']
#                            for engram_id in abstraction['engram_ids']) / len(abstraction['engram_ids'])
#
#         # Create the abstraction node
#         abstraction_node = {
#             'id': f"abs_{int(time.time())}_{abs(hash(str(abstraction['engram_ids']))) % 10000}",
#             'content': summary,
#             'timestamp': time.time(),
#             'source': 'abstraction_engine',
#             'confidence': avg_confidence,
#             'metadata': {
#                 'abstraction_type': abstraction['type'],
#                 'member_ids': abstraction['engram_ids'],
#                 'abstraction_quality': abstraction['abstraction_quality'],
#                 'is_abstract_node': True
#             }
#         }
#
#         if abstraction['type'] == 'topic-cluster':
#             abstraction_node['metadata']['topic'] = abstraction['topic']
#         elif abstraction['type'] == 'temporal-sequence':
#             abstraction_node['metadata']['start_time'] = abstraction['start_time']
#             abstraction_node['metadata']['end_time'] = abstraction['end_time']
#
#         return abstraction_node
#
#     def detect_communities(self, resolution: float = 1.0) -> Dict[str, List[str]]:
#         """
#         Detect communities in the memory graph using the Louvain method.
#         These can be used to build contexts for LLM queries.
#
#         Args:
#             resolution: Resolution parameter for community detection
#                        (higher values = more communities)
#
#         Returns:
#             Dictionary mapping community IDs to lists of engram IDs
#         """
#         print("Detecting knowledge communities...")
#
#         # Convert to undirected graph for community detection
#         undirected = self.engram_graph.to_undirected()
#
#         # Run community detection
#         partition = community_louvain.best_partition(undirected, resolution=resolution)
#
#         # Group nodes by community
#         communities = defaultdict(list)
#         for node, community_id in partition.items():
#             communities[str(community_id)].append(node)
#
#         # Calculate community metrics
#         community_stats = {}
#         for comm_id, members in communities.items():
#             if len(members) < 2:
#                 continue  # Skip singleton communities
#
#             subgraph = self.engram_graph.subgraph(members)
#
#             # Analyze topics in this community
#             topics = defaultdict(int)
#             for node in members:
#                 if node in self.data['engrams']:
#                     topic = self.data['engrams'][node].get('metadata', {}).get('topic', 'unknown')
#                     topics[topic] += 1
#
#             # Find the dominant topic
#             dominant_topic = max(topics.items(), key=lambda x: x[1])[0] if topics else 'unknown'
#             topic_concentration = max(topics.values()) / sum(topics.values()) if sum(topics.values()) > 0 else 0
#
#             community_stats[comm_id] = {
#                 'size': len(members),
#                 'density': nx.density(subgraph),
#                 'dominant_topic': dominant_topic,
#                 'topic_concentration': topic_concentration,
#                 'avg_confidence': sum(self.data['engrams'][node]['confidence']
#                                      for node in members if node in self.data['engrams']) / len(members),
#                 'members': members
#             }
#
#         # Print summary
#         print(f"Detected {len(community_stats)} communities:")
#         for comm_id, stats in community_stats.items():
#             print(f"Community {comm_id}: {stats['size']} engrams, "
#                   f"dominant topic: {stats['dominant_topic']} "
#                   f"({stats['topic_concentration']:.2%} concentration), "
#                   f"density: {stats['density']:.2f}")
#
#         return {comm_id: stats['members'] for comm_id, stats in community_stats.items()}
#
#     def generate_community_contexts(self, communities: Dict[str, List[str]],
#                                    min_size: int = 3,
#                                    max_engrams_per_context: int = 10) -> List[Dict]:
#         """
#         Generate context candidates for LLM requests based on detected communities.
#
#         Args:
#             communities: Dictionary mapping community IDs to lists of engram IDs
#             min_size: Minimum community size to consider
#             max_engrams_per_context: Maximum engrams to include in each context
#
#         Returns:
#             List of context dictionaries
#         """
#         context_candidates = []
#
#         for comm_id, members in communities.items():
#             if len(members) < min_size:
#                 continue
#
#             # Find central nodes in this community
#             subgraph = self.engram_graph.subgraph(members)
#             centrality = nx.pagerank(subgraph)
#
#             # Sort members by centrality
#             sorted_members = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
#
#             # Select top members based on centrality
#             selected_members = [node for node, _ in sorted_members[:max_engrams_per_context]]
#
#             # Analyze topics
#             topics = defaultdict(int)
#             for node in selected_members:
#                 if node in self.data['engrams']:
#                     topic = self.data['engrams'][node].get('metadata', {}).get('topic', 'unknown')
#                     topics[topic] += 1
#
#             # Find the dominant topic
#             dominant_topic = max(topics.items(), key=lambda x: x[1])[0] if topics else 'unknown'
#
#             # Create context
#             context = {
#                 'name': f"Community {comm_id}: {dominant_topic.capitalize()}",
#                 'description': f"Context containing knowledge about {dominant_topic} from community {comm_id}",
#                 'engram_ids': selected_members,
#                 'agent_ids': [],  # Would be populated based on agent expertise
#                 'metadata': {
#                     'community_id': comm_id,
#                     'dominant_topic': dominant_topic,
#                     'topic_distribution': {k: v/len(selected_members) for k, v in topics.items()},
#                     'community_size': len(members),
#                     'selected_size': len(selected_members)
#                 }
#             }
#
#             context_candidates.append(context)
#
#         print(f"Generated {len(context_candidates)} context candidates from communities")
#         return context_candidates
#
#     def analyze_information_flow(self) -> Dict:
#         """
#         Analyze how information flows through the memory graph.
#         Identifies source nodes, sink nodes, and critical pathways.
#
#         Returns:
#             Dictionary of flow analysis results
#         """
#         print("Analyzing information flow in the memory graph...")
#
#         # Identify source nodes (high out-degree, low in-degree)
#         sources = []
#         for node in self.engram_graph.nodes():
#             in_deg = self.engram_graph.in_degree(node)
#             out_deg = self.engram_graph.out_degree(node)
#             if out_deg > 2 and in_deg == 0:
#                 sources.append(node)
#
#         # Identify sink nodes (high in-degree, low out-degree)
#         sinks = []
#         for node in self.engram_graph.nodes():
#             in_deg = self.engram_graph.in_degree(node)
#             out_deg = self.engram_graph.out_degree(node)
#             if in_deg > 2 and out_deg == 0:
#                 sinks.append(node)
#
#         # Identify bridge nodes (high betweenness centrality)
#         betweenness = nx.betweenness_centrality(self.engram_graph)
#         bridges = sorted([(node, score) for node, score in betweenness.items() if score > 0.1],
#                         key=lambda x: x[1], reverse=True)
#
#         # Calculate average path length
#         try:
#             avg_path_length = nx.average_shortest_path_length(self.engram_graph)
#         except nx.NetworkXError:
#             # Graph is not strongly connected
#             avg_path_length = None
#
#         # Analyze connection types
#         connection_types = defaultdict(int)
#         for _, edge_data in self.data['connections'].items():
#             connection_types[edge_data['relationship_type']] += 1
#
#         results = {
#             'source_nodes': [
#                 {'id': node, 'content': self.data['engrams'][node]['content'][:50] + '...'}
#                 for node in sources[:5]
#             ],
#             'sink_nodes': [
#                 {'id': node, 'content': self.data['engrams'][node]['content'][:50] + '...'}
#                 for node in sinks[:5]
#             ],
#             'bridge_nodes': [
#                 {'id': node, 'betweenness': score,
#                  'content': self.data['engrams'][node]['content'][:50] + '...'}
#                 for node, score in bridges[:5]
#             ],
#             'average_path_length': avg_path_length,
#             'connection_types': dict(connection_types),
#             'diameter': nx.diameter(self.engram_graph) if nx.is_strongly_connected(self.engram_graph) else None,
#             'is_dag': nx.is_directed_acyclic_graph(self.engram_graph),  # Check if knowledge forms a DAG
#         }
#
#         print(f"Information flow analysis complete. Found {len(sources)} source nodes, "
#               f"{len(sinks)} sink nodes, and {len(bridges)} bridge nodes.")
#
#         return results
#
#     def visualize_memory_structure(self, highlight_communities: bool = True):
#         """
#         Create a visualization of the memory graph structure,
#         optionally highlighting communities.
#         """
#         print("Generating memory graph visualization...")
#
#         # Use only engram nodes and their connections for clarity
#         G = self.engram_graph.copy()
#
#         # Remove isolated nodes for cleaner visualization
#         G.remove_nodes_from(list(nx.isolates(G)))
#
#         if len(G) > 100:
#             print(f"Warning: Graph has {len(G)} nodes, visualization may be cluttered")
#
#         plt.figure(figsize=(12, 10))
#
#         # Node positions
#         pos = nx.spring_layout(G, seed=42)
#
#         if highlight_communities:
#             # Detect communities
#             partition = community_louvain.best_partition(G.to_undirected())
#
#             # Convert community IDs to colors
#             cmap = plt.colormaps['tab20']
#             colors = [cmap(partition[node]) for node in G.nodes()]
#
#             # Draw nodes colored by community
#             nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=100, alpha=0.8)
#         else:
#             # Color nodes by confidence
#             confidence_values = [self.data['engrams'][node]['confidence'] for node in G.nodes()]
#             nx.draw_networkx_nodes(G, pos, node_color=confidence_values,
#                                   cmap=plt.cm.YlOrRd, node_size=100, alpha=0.8)
#
#         # Draw edges with alpha based on weight
#         edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
#         nx.draw_networkx_edges(G, pos, alpha=0.3, width=edge_weights)
#
#         # Draw edge labels for a subset of edges
#         if len(G.edges()) < 50:  # Only show edge labels if not too many
#             edge_labels = {(u, v): G[u][v]['relationship_type']
#                           for u, v in G.edges() if 'relationship_type' in G[u][v]}
#             nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
#
#         # Add a colorbar if not highlighting communities
#         if not highlight_communities:
#             sm = plt.cm.ScalarMappable(cmap=plt.cm.YlOrRd)
#             sm.set_array([])
#             plt.colorbar(sm, label='Confidence')
#
#         plt.title('Memory Graph Structure Analysis')
#         plt.axis('off')
#         plt.tight_layout()
#         plt.show()
#
#     def recommend_memory_optimizations(self) -> List[Dict]:
#         """
#         Provide recommendations for optimizing the memory graph.
#
#         Returns:
#             List of recommendation dictionaries
#         """
#         recommendations = []
#
#         # Get basic metrics
#         avg_confidence = np.mean([e['confidence'] for e in self.data['engrams'].values()])
#         num_isolated = len(list(nx.isolates(self.engram_graph)))
#
#         # Check density
#         density = nx.density(self.engram_graph)
#         if density < 0.05:
#             recommendations.append({
#                 'type': 'connectivity',
#                 'issue': 'Low graph density',
#                 'description': 'Memory graph is very sparse, which may indicate isolated knowledge islands',
#                 'recommendation': 'Increase connections between related engrams to improve knowledge integration',
#                 'severity': 'medium'
#             })
#
#         # Check for isolated nodes
#         if num_isolated > len(self.data['engrams']) * 0.1:  # More than 10% isolated
#             recommendations.append({
#                 'type': 'connectivity',
#                 'issue': 'High number of isolated engrams',
#                 'description': f'Found {num_isolated} engrams with no connections',
#                 'recommendation': 'Either connect these to the knowledge graph or consider them for pruning',
#                 'severity': 'high'
#             })
#
#         # Check for cycles (which might indicate circular reasoning)
#         try:
#             cycles = list(nx.simple_cycles(self.engram_graph))
#             if len(cycles) > 0:
#                 recommendations.append({
#                     'type': 'structure',
#                     'issue': 'Circular knowledge references',
#                     'description': f'Found {len(cycles)} cycles in the knowledge graph',
#                     'recommendation': 'Review these cycles to ensure they represent valid circular references',
#                     'examples': [cycle for cycle in cycles[:3]]
#                 })
#         except:
#             pass  # Skip if algorithm doesn't converge
#
# # Check for low confidence nodes
#         low_conf_count = sum(1 for e in self.data['engrams'].values() if e['confidence'] < 0.6)
#         if low_conf_count > len(self.data['engrams']) * 0.2:  # More than 20% low confidence
#             recommendations.append({
#                 'type': 'quality',
#                 'issue': 'High proportion of low-confidence engrams',
#                 'description': f'Found {low_conf_count} engrams with confidence below 0.6',
#                 'recommendation': 'Review and validate these engrams or consider pruning',
#                 'severity': 'medium'
#             })
#
#         # Check community structure
#         communities = self.detect_communities()
#         if len(communities) < 2:
#             recommendations.append({
#                 'type': 'structure',
#                 'issue': 'Poor community structure',
#                 'description': 'Knowledge graph lacks clear topical communities',
#                 'recommendation': 'Organize knowledge into clearer topical clusters',
#                 'severity': 'low'
#             })
#         elif len(communities) > len(self.data['engrams']) * 0.4:
#             recommendations.append({
#                 'type': 'structure',
#                 'issue': 'Fragmented community structure',
#                 'description': 'Knowledge is split into too many small communities',
#                 'recommendation': 'Create more connections between related communities',
#                 'severity': 'medium'
#             })
#
#         # Check temporal distribution
#         timestamps = [e['timestamp'] for e in self.data['engrams'].values()]
#         time_range = max(timestamps) - min(timestamps) if timestamps else 0
#         if time_range > 0:
#             recent_count = sum(1 for t in timestamps if max(timestamps) - t < time_range * 0.1)
#             if recent_count < len(timestamps) * 0.05:  # Less than 5% are recent
#                 recommendations.append({
#                     'type': 'recency',
#                     'issue': 'Stale knowledge base',
#                     'description': 'Very few recent engrams in the memory graph',
#                     'recommendation': 'Add fresh knowledge to keep the memory current',
#                     'severity': 'medium'
#                 })
#
#         # Check abstraction potential
#         abstractions = self.identify_subgraph_abstractions()
#         if len(abstractions) > 10:
#             recommendations.append({
#                 'type': 'abstraction',
#                 'issue': 'High abstraction potential',
#                 'description': f'Found {len(abstractions)} candidate subgraphs for abstraction',
#                 'recommendation': 'Consider creating abstract nodes to simplify the knowledge graph',
#                 'severity': 'low',
#                 'top_candidates': abstractions[:3]
#             })
#
#         # Check for hub nodes (potential knowledge bottlenecks)
#         in_degree = sorted([(node, self.engram_graph.in_degree(node)) for node in self.engram_graph.nodes()],
#                           key=lambda x: x[1], reverse=True)
#         if in_degree and in_degree[0][1] > len(self.engram_graph) * 0.3:  # One node connects to >30% of graph
#             recommendations.append({
#                 'type': 'structure',
#                 'issue': 'Knowledge bottleneck',
#                 'description': f'Node {in_degree[0][0][:8]}... is referenced by {in_degree[0][1]} other engrams',
#                 'recommendation': 'Consider breaking down this hub engram into more specific components',
#                 'severity': 'medium'
#             })
#
#         return recommendations
#
#     def memory_health_report(self) -> Dict:
#         """
#         Generate a comprehensive health report for the memory graph.
#
#         Returns:
#             Dictionary with health metrics and recommendations
#         """
#         print("Generating memory health report...")
#
#         # Basic metrics
#         metrics = {
#             'engram_count': len(self.data['engrams']),
#             'connection_count': len(self.data['connections']),
#             'collection_count': len(self.data['collections']),
#             'context_count': len(self.data['contexts']),
#             'agent_count': len(self.data['agents']),
#             'average_confidence': np.mean([e['confidence'] for e in self.data['engrams'].values()]),
#             'graph_density': nx.density(self.engram_graph),
#             'isolated_nodes': len(list(nx.isolates(self.engram_graph))),
#             'connectivity': nx.number_strongly_connected_components(self.engram_graph),
#             'reciprocity': nx.reciprocity(self.engram_graph)  # Measure of mutual connections
#         }
#
#         # Age analysis
#         timestamps = [e['timestamp'] for e in self.data['engrams'].values()]
#         if timestamps:
#             current_time = time.time()
#             metrics['oldest_engram_age_days'] = (current_time - min(timestamps)) / (60 * 60 * 24)
#             metrics['newest_engram_age_days'] = (current_time - max(timestamps)) / (60 * 60 * 24)
#             metrics['average_engram_age_days'] = (current_time - np.mean(timestamps)) / (60 * 60 * 24)
#
#         # Community structure
#         communities = self.detect_communities()
#         metrics['community_count'] = len(communities)
#         metrics['community_sizes'] = [len(members) for members in communities.values()]
#
#         # Generate recommendations
#         recommendations = self.recommend_memory_optimizations()
#
#         # Forgetting candidates
#         forget_candidates = self.analyze_forgetting_candidates(top_n=5)
#         forget_candidate_ids = forget_candidates['engram_id'].tolist()
#
#         # Abstraction candidates
#         abstraction_candidates = self.identify_subgraph_abstractions()[:3]
#
#         # Overall health score (simple heuristic)
#         health_score = 0.0
#
#         # Base score from density and connectivity
#         health_score += metrics['graph_density'] * 20  # 0-1 scale to 0-20
#
#         # Penalize for isolated nodes
#         isolated_ratio = metrics['isolated_nodes'] / max(1, metrics['engram_count'])
#         health_score -= isolated_ratio * 15  # Penalty of up to 15 points
#
#         # Bonus for community structure
#         if 3 <= metrics['community_count'] <= 10:
#             health_score += 10  # Good community structure
#         elif metrics['community_count'] > 10:
#             health_score += 5   # Acceptable community structure
#
#         # Bonus for reciprocity
#         health_score += metrics['reciprocity'] * 10  # 0-1 scale to 0-10
#
#         # Confidence bonus
#         if metrics['average_confidence'] > 0.8:
#             health_score += 10
#         elif metrics['average_confidence'] > 0.6:
#             health_score += 5
#
#         # Cap health score
#         health_score = max(0, min(100, health_score))
#
#         metrics['health_score'] = health_score
#
#         # Health status
#         if health_score >= 80:
#             health_status = "Excellent"
#         elif health_score >= 60:
#             health_status = "Good"
#         elif health_score >= 40:
#             health_status = "Fair"
#         elif health_score >= 20:
#             health_status = "Poor"
#         else:
#             health_status = "Critical"
#
#         metrics['health_status'] = health_status
#
#         return {
#             'metrics': metrics,
#             'recommendations': recommendations,
#             'forget_candidates': forget_candidate_ids,
#             'abstraction_candidates': abstraction_candidates,
#             'timestamp': time.time()
#         }
#
#     def export_optimized_graph(self, output_file: str):
#         """
#         Export an optimized version of the memory graph based on recommendations.
#         This includes pruning forgettable nodes and creating abstractions.
#
#         Args:
#             output_file: Path to save the optimized graph
#         """
#         print("Creating optimized memory graph...")
#
#         # Create a copy of the original data
#         optimized_data = {
#             'engrams': dict(self.data['engrams']),
#             'connections': dict(self.data['connections']),
#             'collections': dict(self.data['collections']),
#             'agents': dict(self.data['agents']),
#             'contexts': dict(self.data['contexts'])
#         }
#
#         # 1. Remove top forgetting candidates (lowest importance nodes)
#         forget_candidates = self.analyze_forgetting_candidates(top_n=int(len(self.data['engrams']) * 0.1))
#         forget_ids = forget_candidates['engram_id'].tolist()
#
#         print(f"Pruning {len(forget_ids)} low-importance engrams")
#
#         # Remove the engrams
#         for engram_id in forget_ids:
#             if engram_id in optimized_data['engrams']:
#                 del optimized_data['engrams'][engram_id]
#
#         # Remove connections involving these engrams
#         connections_to_remove = []
#         for conn_id, conn in optimized_data['connections'].items():
#             if conn['source_id'] in forget_ids or conn['target_id'] in forget_ids:
#                 connections_to_remove.append(conn_id)
#
#         for conn_id in connections_to_remove:
#             if conn_id in optimized_data['connections']:
#                 del optimized_data['connections'][conn_id]
#
#         # Update collections and contexts
#         for coll_id, coll in optimized_data['collections'].items():
#             coll['engram_ids'] = [e_id for e_id in coll['engram_ids'] if e_id not in forget_ids]
#
#         for ctx_id, ctx in optimized_data['contexts'].items():
#             ctx['engram_ids'] = [e_id for e_id in ctx['engram_ids'] if e_id not in forget_ids]
#
#         # 2. Create abstractions for suitable subgraphs
#         abstractions = self.identify_subgraph_abstractions()
#
#         # Filter to high-quality abstractions
#         good_abstractions = [abs for abs in abstractions if abs['abstraction_quality'] > 0.7][:5]
#
#         print(f"Creating {len(good_abstractions)} abstraction nodes")
#
#         # Track which engrams become part of abstractions
#         abstracted_engrams = set()
#
#         # Create abstraction nodes
#         for abstraction in good_abstractions:
#             # Create the abstraction node
#             abstraction_node = self.create_abstraction_node(abstraction)
#
#             # Add to optimized data
#             node_id = abstraction_node['id']
#             optimized_data['engrams'][node_id] = abstraction_node
#
#             # Connect the abstraction to other nodes that were connected to its members
#             for member_id in abstraction['engram_ids']:
#                 abstracted_engrams.add(member_id)
#
#                 # Find connections to/from this member to nodes outside the abstraction
#                 for conn_id, conn in self.data['connections'].items():
#                     if conn['source_id'] == member_id and conn['target_id'] not in abstraction['engram_ids']:
#                         # Create a new connection from abstraction to external node
#                         new_conn_id = f"abs_conn_{int(time.time())}_{len(optimized_data['connections'])}"
#                         optimized_data['connections'][new_conn_id] = {
#                             'source_id': node_id,
#                             'target_id': conn['target_id'],
#                             'relationship_type': conn['relationship_type'],
#                             'weight': conn['weight'],
#                             'metadata': {'abstracted_from': member_id}
#                         }
#
#                     elif conn['target_id'] == member_id and conn['source_id'] not in abstraction['engram_ids']:
#                         # Create a new connection from external node to abstraction
#                         new_conn_id = f"abs_conn_{int(time.time())}_{len(optimized_data['connections'])}"
#                         optimized_data['connections'][new_conn_id] = {
#                             'source_id': conn['source_id'],
#                             'target_id': node_id,
#                             'relationship_type': conn['relationship_type'],
#                             'weight': conn['weight'],
#                             'metadata': {'abstracted_to': member_id}
#                         }
#
#         # 3. Update collections and contexts to include abstractions
#         # For each abstracted engram, add its abstraction to any collection it was in
#         for coll_id, coll in optimized_data['collections'].items():
#             for abstraction in good_abstractions:
#                 abs_id = abstraction['id'] if 'id' in abstraction else None
#                 if not abs_id:
#                     continue
#
#                 # If any member of this abstraction is in the collection, add the abstraction
#                 if any(member_id in coll['engram_ids'] for member_id in abstraction['engram_ids']):
#                     if abs_id not in coll['engram_ids']:
#                         coll['engram_ids'].append(abs_id)
#
#         # Similar for contexts
#         for ctx_id, ctx in optimized_data['contexts'].items():
#             for abstraction in good_abstractions:
#                 abs_id = abstraction['id'] if 'id' in abstraction else None
#                 if not abs_id:
#                     continue
#
#                 # If any member of this abstraction is in the context, add the abstraction
#                 if any(member_id in ctx['engram_ids'] for member_id in abstraction['engram_ids']):
#                     if abs_id not in ctx['engram_ids']:
#                         ctx['engram_ids'].append(abs_id)
#
#         # 4. Option: Remove the abstracted engrams to keep only the abstractions
#         # This is optional depending on whether you want to keep the original engrams
#         # Uncomment to remove them
#         """
#         for engram_id in abstracted_engrams:
#             if engram_id in optimized_data['engrams']:
#                 del optimized_data['engrams'][engram_id]
#         """
#
#         # Save the optimized graph
#         with open(output_file, 'w') as f:
#             json.dump(optimized_data, f, indent=2)
#
#         print(f"Optimized memory graph saved to {output_file}")
#         print(f"Original graph: {len(self.data['engrams'])} engrams, {len(self.data['connections'])} connections")
#         print(f"Optimized graph: {len(optimized_data['engrams'])} engrams, {len(optimized_data['connections'])} connections")
#
# # Main function to demonstrate the analyzer
# def main():
#     import argparse
#
#     parser = argparse.ArgumentParser(description='Analyze memory graph structure')
#     parser.add_argument('--input', required=True, help='Input memory graph JSON file')
#     parser.add_argument('--output', help='Output file for optimized graph')
#     parser.add_argument('--visualize', action='store_true', help='Visualize the memory graph')
#     parser.add_argument('--health-report', action='store_true', help='Generate health report')
#     parser.add_argument('--analyze-forgetting', action='store_true', help='Analyze forgetting candidates')
#     parser.add_argument('--find-abstractions', action='store_true', help='Find abstraction candidates')
#     parser.add_argument('--detect-communities', action='store_true', help='Detect knowledge communities')
#     parser.add_argument('--analyze-flow', action='store_true', help='Analyze information flow')
#     parser.add_argument('--optimize', action='store_true', help='Create optimized graph')
#
#     args = parser.parse_args()
#
#     # Create analyzer
#     analyzer = MemoryGraphAnalyzer(args.input)
#
#     # Run requested analyses
#     if args.analyze_forgetting:
#         forget_candidates = analyzer.analyze_forgetting_candidates()
#         print("\nTop forgetting candidates:")
#         print(forget_candidates[['content', 'forgetting_score', 'centrality', 'recency']])
#
#     if args.find_abstractions:
#         abstractions = analyzer.identify_subgraph_abstractions()
#         print("\nTop abstraction candidates:")
#         for i, abs in enumerate(abstractions[:3]):
#             print(f"\nAbstraction {i+1} ({abs['type']}, quality: {abs['abstraction_quality']:.2f}):")
#             for j, (engram_id, content) in enumerate(list(abs['engrams'].items())[:3]):
#                 print(f"  - {content[:70]}...")
#             if len(abs['engrams']) > 3:
#                 print(f"  - And {len(abs['engrams']) - 3} more engrams")
#
#     if args.detect_communities:
#         communities = analyzer.detect_communities()
#         # Results already printed in the method
#
#     if args.analyze_flow:
#         flow = analyzer.analyze_information_flow()
#         print("\nInformation Flow Analysis:")
#         print(f"- Average path length: {flow['average_path_length']}")
#         print(f"- Is DAG: {flow['is_dag']}")
#         print("- Connection types:")
#         for conn_type, count in flow['connection_types'].items():
#             print(f"  * {conn_type}: {count}")
#         print("- Top source nodes:")
#         for node in flow['source_nodes']:
#             print(f"  * {node['content']}")
#         print("- Top sink nodes:")
#         for node in flow['sink_nodes']:
#             print(f"  * {node['content']}")
#
#     if args.health_report:
#         report = analyzer.memory_health_report()
#
#         print("\n=== MEMORY GRAPH HEALTH REPORT ===")
#         print(f"Status: {report['metrics']['health_status']} ({report['metrics']['health_score']:.1f}/100)")
#         print(f"Size: {report['metrics']['engram_count']} engrams, {report['metrics']['connection_count']} connections")
#         print(f"Density: {report['metrics']['graph_density']:.3f}")
#         print(f"Communities: {report['metrics']['community_count']}")
#         print(f"Isolated nodes: {report['metrics']['isolated_nodes']} ({report['metrics']['isolated_nodes']/max(1, report['metrics']['engram_count']):.1%} of total)")
#
#         print("\nTop Recommendations:")
#         for i, rec in enumerate(report['recommendations'][:3]):
#             print(f"{i+1}. {rec['issue']}: {rec['description']}")
#             print(f"   Recommendation: {rec['recommendation']}")
#
#     if args.visualize:
#         analyzer.visualize_memory_structure()
#
#     if args.optimize and args.output:
#         analyzer.export_optimized_graph(args.output)
#
# if __name__ == "__main__":
#     main()

# Run various analyses
!python memory_analysis.py --input memory_graph.json --analyze-forgetting --health-report

# Generate visualizations
!python memory_analysis.py --input memory_graph.json --visualize --detect-communities

# Commented out IPython magic to ensure Python compatibility.
#@title Visualize
def visualize_colab_graph(graph_file):
    """
    Display memory graph visualization properly in Colab.
    """
    import networkx as nx
    import matplotlib.pyplot as plt
    import json
    import community as community_louvain

    # Set matplotlib to inline mode
#     %matplotlib inline

    # Load the graph data
    with open(graph_file, 'r') as f:
        data = json.load(f)

    # Create the engram graph
    G = nx.DiGraph()

    # Add engram nodes
    for engram_id in data['engrams']:
        G.add_node(engram_id)

    # Add connection edges
    for conn_id, conn in data['connections'].items():
        if conn['source_id'] in data['engrams'] and conn['target_id'] in data['engrams']:
            G.add_edge(
                conn['source_id'],
                conn['target_id'],
                weight=conn['weight']
            )

    # Remove isolated nodes for cleaner visualization
    G.remove_nodes_from(list(nx.isolates(G)))

    # Use spring layout for node positions
    pos = nx.spring_layout(G, seed=42)

    # Detect communities for coloring
    partition = community_louvain.best_partition(G.to_undirected())

    # Create a new figure
    plt.figure(figsize=(15, 12))

    # Get colors from the tab20 colormap
    # Fix the deprecated get_cmap function
    from matplotlib import colormaps
    cmap = colormaps['tab20']
    colors = [cmap(partition[node] % cmap.N) for node in G.nodes()]

    # Draw nodes colored by community
    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=100, alpha=0.8)

    # Draw edges with alpha based on weight
    edge_weights = [G[u][v].get('weight', 1.0) for u, v in G.edges()]
    nx.draw_networkx_edges(G, pos, alpha=0.3, width=edge_weights)

    # Draw labels if not too many nodes
    if len(G) < 50:
        labels = {node: node[:8] for node in G.nodes()}
        nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)

    plt.title('Memory Graph Community Structure')
    plt.axis('off')

    # Make sure to call show() explicitly
    plt.tight_layout()
    plt.show()

    print(f"Displayed graph with {len(G.nodes())} nodes and {len(G.edges())} edges")

    # Return community info
    communities = {}
    for node, community_id in partition.items():
        if community_id not in communities:
            communities[community_id] = []
        communities[community_id].append(node)

    print(f"Detected {len(communities)} communities:")
    for comm_id, members in communities.items():
        print(f"Community {comm_id}: {len(members)} nodes")

# Call the function
visualize_colab_graph("memory_graph.json")

# Create an optimized graph
!python memory_analysis.py --input memory_graph.json --optimize --output optimized_memory.json